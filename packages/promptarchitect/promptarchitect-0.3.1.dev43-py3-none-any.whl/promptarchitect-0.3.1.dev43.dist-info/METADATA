Metadata-Version: 2.3
Name: promptarchitect
Version: 0.3.1.dev43
Summary: Engineered prompts are carefully designed inputs for AI models to ensure consistent and reliable outputs, playing a crucial role in automated processes involving large language models.
Author-email: Joop Snijder <joop@aigency.com>
License-File: LICENSE
Requires-Python: >=3.11
Requires-Dist: anthropic>=0.32.0
Requires-Dist: bs4>=0.0.2
Requires-Dist: click>=8.1.3
Requires-Dist: coloredlogs==15.0.1
Requires-Dist: markdown==3.6
Requires-Dist: openai>=1.37.1
Requires-Dist: python-dotenv==1.0.1
Requires-Dist: python-frontmatter>=1.0.0
Requires-Dist: retry==0.9.2
Requires-Dist: typer>=0.12.3
Description-Content-Type: text/markdown

# PromptArchitect

## About Engineered Prompts PromptArchitect

### Engineered Prompts: Structured Inputs for Automated Processes

In the rapidly evolving landscape of artificial intelligence, the concept of an "engineered prompt" is gaining prominence, particularly in environments that leverage large language models (LLMs) and other AI systems. Engineered prompts are meticulously crafted inputs designed to interact with AI models in a way that ensures consistent and reliable outputs. These prompts are not just queries but structured tools that are integral to the automated processes in which they function.

## Definition and Purpose

An engineered prompt is a carefully designed input that is used to generate a specific type of response from an AI model. Unlike casual or ad-hoc prompts, engineered prompts are developed through a rigorous process that considers the nuances of the model’s language understanding and output capabilities. They are akin to code in software development, serving as a fundamental component that interacts with the AI to execute specific tasks reliably.

## Characteristics of Engineered Prompts

- **Precision and Clarity**: Engineered prompts are precise, unambiguous, and tailored to elicit a specific type of response or behavior from an AI model.
- **Reusability**: These prompts are designed to be reusable across similar tasks or models, ensuring efficiency and consistency in automated processes.
- **Scalability**: Engineered prompts can be scaled or modified according to different requirements or in response to changes in the AI model’s behavior.

## Development and Maintenance

Just like any software code, engineered prompts require a structured development and maintenance process to ensure they remain effective and safe for use:

- **Versioning**: Keeping track of different versions of prompts is crucial, especially as models and requirements evolve. Versioning allows developers to manage changes systematically, revert to previous versions if needed, and understand the evolution of prompt effectiveness over time.
- **Documentation**: Comprehensive documentation is essential for engineered prompts. It should detail the design rationale, expected outputs, model compatibility, and any dependencies. This documentation is vital for both current use and future modifications.
- **Testing and Validation**: Rigorous testing is a cornerstone of prompt development. This includes unit testing to verify prompt functionality, integration testing to ensure compatibility with the AI model, and validation testing to confirm that the prompt generates the expected outputs.
- **Performance Tests**: Performance testing evaluates how well the prompt works in terms of speed and resource utilization, ensuring that the prompt is efficient even at scale.
- **Regression Testing**: This is particularly critical when the underlying AI model is updated or when switching to a model from a different provider. Regression tests help verify that updates or changes do not negatively affect the performance of the prompt.

## Use Cases

Engineered prompts are used in diverse fields such as customer service, content generation, automated programming help, and more. In each case, the prompt acts as a bridge between the user’s needs and the model’s capabilities, facilitating a controlled and predictable AI interaction.

## New Feature: Default Model Configuration

PromptArchitect now supports specifying a default model per provider. This ensures that when no model is specified in a prompt file, the default model for the provider is used.

### Configuration

Update your provider configuration files to include a `default_model` key. Below is an example of a provider configuration file:

```json
{
    "gpt-3.5-turbo-instruct": {
        "input_tokens": 0.0000015,
        "output_tokens": 0.000002
    },
    "gpt-4o-mini": {
        "input_tokens": 0.00000015,
        "output_tokens": 0.0000006
    },
    "gpt-4o": {
        "input_tokens": 0.00005,
        "output_tokens": 0.000015
    },
    "default_model": "gpt-4o"
}
```

## More Information

- [Testing Language Models (and Prompts) Like We Test Software](https://towardsdatascience.com/testing-large-language-models-like-we-test-software-92745d28a359)
- [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf)

## Installation

Use the following command to install PromptArchitect:

```bash
pip install promptarchitect
```

## Usage

Below is an example of how to use the EngineeredPrompt class from the PromptArchitect module:

```python
from promptarchitect import EngineeredPrompt

# Initialize the EngineeredPrompt
prompt = EngineeredPrompt(prompt_file_path='path_to_prompt_file.prompt', output_path='output_directory')

# Execute the prompt
response = prompt.execute(input_file='path_to_input_file.txt')
print(response)
```

## License

This project is licensed under the MIT License. See the LICENSE file for details.

## Contributing

We welcome contributions! Please see the CONTRIBUTING.md for more details.

## Contact

For any questions or issues, please open an issue on this GitHub repository or contact the maintainers.
