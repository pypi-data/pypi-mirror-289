"""Differential Evolution (DE) Algorithm.

This module implements the Differential Evolution (DE) algorithm. DE is a
population-based metaheuristic optimization algorithm developed
by R. Storn and K. Price in 1997. It is simple, robust, and has proven to be effective
for a wide range of optimization problems.

DE generates new candidate solutions by combining existing ones according to its simple
formulae. For each iteration/generation, new solutions are generated by adding the
weighted difference between two solutions to a third solution. If the generated
solution has better fitness than the current solution in consideration, it replaces
the current solution.

DE is particularly useful for numerical optimization problems that are computationally
intensive, non-differentiable, noisy, discontinuous, and multimodal.

Example:
    optimizer = DifferentialEvolution(func=objective_function, lower_bound=-10,
    upper_bound=10, dim=2, population_size=50, max_iter=1000)
    best_solution, best_fitness = optimizer.optimize()

Attributes:
    func (Callable): The objective function to optimize.
    lower_bound (float): The lower bound of the search space.
    upper_bound (float): The upper bound of the search space.
    dim (int): The dimension of the search space.
    population_size (int): The size of the population (candidate solutions).
    max_iter (int): The maximum number of iterations.

Methods:
    optimize(): Perform the DE optimization.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np

from opt.abstract_optimizer import AbstractOptimizer
from opt.benchmark.functions import shifted_ackley


if TYPE_CHECKING:
    from collections.abc import Callable

    from numpy import ndarray


class DifferentialEvolution(AbstractOptimizer):
    """Differential Evolution optimizer.

    This optimizer uses the Differential Evolution algorithm to search for the
    optimal solution of a given function within a specified search space.

    Parameters:
        func (Callable[[ndarray], float]): The objective function to be minimized.
        lower_bound (float): The lower bound of the search space.
        upper_bound (float): The upper bound of the search space.
        dim (int): The dimensionality of the search space.
        population_size (int, optional): The size of the population. Default is 100.
        max_iter (int, optional): The maximum number of iterations. Default is 1000.
        F (float, optional): The differential weight parameter. Default is 0.5.
        CR (float, optional): The crossover probability parameter. Default is 0.7.
        seed (Optional[int], optional): The random seed for reproducibility. Default is None.
    """

    def __init__(
        self,
        func: Callable[[ndarray], float],
        lower_bound: float,
        upper_bound: float,
        dim: int,
        population_size: int = 100,
        max_iter: int = 1000,
        F: float = 0.5,
        CR: float = 0.7,
        seed: int | None = None,
    ) -> None:
        """Initialize the DifferentialEvolution class."""
        super().__init__(
            func=func,
            lower_bound=lower_bound,
            upper_bound=upper_bound,
            dim=dim,
            max_iter=max_iter,
            seed=seed,
            population_size=population_size,
        )
        self.F = F
        self.CR = CR

    def search(self) -> tuple[np.ndarray, float]:
        """Perform the differential evolution search.

        Returns:
            Tuple[np.ndarray, float]: A tuple containing the best solution found and its fitness value.
        """
        # Initialize population and fitness
        population = np.random.default_rng(self.seed).uniform(
            self.lower_bound, self.upper_bound, (self.population_size, self.dim)
        )
        fitness = np.apply_along_axis(self.func, 1, population)

        # Main loop
        for _ in range(self.max_iter):
            self.seed += 1
            for i in range(self.population_size):
                self.seed += 1
                # Mutation
                indices = [idx for idx in range(self.population_size) if idx != i]
                x_a, x_b, x_c = population[
                    np.random.default_rng(self.seed + 1).choice(
                        indices, 3, replace=False
                    )
                ]
                mutant = x_a + self.F * (x_b - x_c)
                mutant = np.clip(mutant, self.lower_bound, self.upper_bound)

                # Crossover
                cross_points = (
                    np.random.default_rng(self.seed + 2).random(self.dim) < self.CR
                )
                if not np.any(cross_points):
                    cross_points[
                        np.random.default_rng(self.seed + 3).integers(0, self.dim)
                    ] = True
                trial = np.where(cross_points, mutant, population[i])

                # Selection
                trial_fitness = self.func(trial)
                if trial_fitness < fitness[i]:
                    fitness[i] = trial_fitness
                    population[i] = trial

        # Get best solution
        best_index = np.argmin(fitness)
        best_solution = population[best_index]
        best_fitness = fitness[best_index]

        return best_solution, best_fitness


if __name__ == "__main__":
    optimizer = DifferentialEvolution(
        func=shifted_ackley, lower_bound=-32.768, upper_bound=+32.768, dim=2
    )
    best_solution, best_fitness = optimizer.search()
    print(f"Best solution found: {best_solution}")
    print(f"Best fitness found: {best_fitness}")
