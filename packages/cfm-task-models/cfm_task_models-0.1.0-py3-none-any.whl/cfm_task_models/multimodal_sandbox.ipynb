{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mmdet.apis import init_detector\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmcv/cnn/bricks/transformer.py:441: DeprecationWarning: The arguments `dropout` in MultiheadAttention has been deprecated, now you can separately set `attn_drop`(float), proj_drop(float), and `dropout_layer`(dict) \n",
      "  warnings.warn(\n",
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:326: ResourceWarning: unclosed file <_io.BufferedRandom name='/tmp/tmpnc_hcvoy/tmph5gu4omw.py'>\n",
      "  super().__setattr__('_is_full_backward_hook', None)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/apis/inference.py:70: UserWarning: checkpoint is None, use COCO classes by default.\n",
      "  warnings.warn('checkpoint is None, use COCO classes by default.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language_model.language_backbone.body.model.embeddings.position_embeddings.weight']\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmcv/cnn/bricks/transformer.py:441: DeprecationWarning: The arguments `dropout` in MultiheadAttention has been deprecated, now you can separately set `attn_drop`(float), proj_drop(float), and `dropout_layer`(dict) \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ../checkpoints/multimodal/mm_grounding_dino.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: language_model.language_backbone.body.model.embeddings.position_ids\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_init = init_detector('../configs/multimodal/mm_grounding_dino_coco.py', device='cpu')\n",
    "\n",
    "print([k for k in model_init.state_dict().keys() if 'language' in k and 'position' in k])\n",
    "saved_checkpoint = torch.load('../checkpoints/multimodal/mm_grounding_dino.pth')\n",
    "model_init.load_state_dict(saved_checkpoint['state_dict'], strict = False)\n",
    "print(all([torch.all(saved_checkpoint['state_dict'][k] == model_init.state_dict()[k]) for k in saved_checkpoint['state_dict'].keys() if k!='language_model.language_backbone.body.model.embeddings.position_ids']))\n",
    "\n",
    "model_init_2 = init_detector('../configs/multimodal/mm_grounding_dino_coco.py','../checkpoints/multimodal/mm_grounding_dino.pth', device='cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.runner import Runner\n",
    "from mmdet.structures import DetDataSample\n",
    "from typing import Union\n",
    "from torch import Tensor\n",
    "def to_device(data_sample:Union[dict,Tensor], device):\n",
    "    if isinstance(data_sample,dict):\n",
    "        for k,v in data_sample.items():\n",
    "            if isinstance(v, (Tensor, DetDataSample)):\n",
    "                data_sample[k] = v.to(device)\n",
    "                # print(f'{k} was sent to {device}')\n",
    "            elif isinstance(v, list):\n",
    "                new_v = [to_device(v2, device) for v2 in v]\n",
    "                data_sample[k] = new_v\n",
    "                # print(f'components of {k} were sent to {device}')\n",
    "            else:\n",
    "                print(f'{k} could not be sent to {device}')\n",
    "    elif isinstance(data_sample, (Tensor, DetDataSample)):\n",
    "        data_sample = data_sample.to(device)\n",
    "        # print(f'input was sent to {device}')\n",
    "    elif isinstance(data_sample, list):\n",
    "        data_sample = [to_device(v, device) for v in data_sample]\n",
    "        # print(f'components of input were sent to {device}')\n",
    "    else:\n",
    "        # print(type(data_sample))\n",
    "        print(f'input could not be sent to {device}')\n",
    "    return data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'CocoDataset', 'data_root': '../data/coco/', 'ann_file': 'annotations/instances_val2017.json', 'data_prefix': {'img': 'val2017/'}, 'test_mode': True, 'pipeline': [{'type': 'LoadImageFromFile', 'backend_args': None, 'imdecode_backend': 'pillow'}, {'type': 'FixScaleResize', 'scale': (800, 1333), 'keep_ratio': True, 'backend': 'pillow'}, {'type': 'LoadAnnotations', 'with_bbox': True}, {'type': 'PackDetInputs', 'meta_keys': ('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor', 'text', 'custom_entities', 'tokens_positive')}], 'backend_args': None, 'return_classes': True}\n",
      "annotations/instances_val2017.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.19s)\n",
      "creating index...\n",
      "index created!\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "from mmengine.config import Config\n",
    "# cfg = Config.fromfile('../configs/multimodal/mm_grounding_dino_coco.py')\n",
    "# print(cfg)\n",
    "cfg_tr = model_init_2.cfg.copy()\n",
    "cfg_tr['val_dataloader']['dataset']['data_root'] = '../data/coco/'\n",
    "cfg_tr['test_dataloader']['dataset']['data_root'] = '../data/coco/'\n",
    "print(cfg_tr['test_dataloader']['dataset'])\n",
    "# cfg_tr['train_dataloader']['batch_size'] = 8\n",
    "# cfg_tr['train_dataloader']['num_workers'] = 1\n",
    "\n",
    "# train_dataloader = Runner.build_dataloader(cfg_tr['train_dataloader']) \n",
    "\n",
    "print(cfg_tr['val_dataloader']['dataset']['ann_file'])\n",
    "val_dataloader = Runner.build_dataloader(cfg_tr['val_dataloader'])\n",
    "\n",
    "test_dataloader = Runner.build_dataloader(cfg_tr['test_dataloader'])\n",
    "print(len(val_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.20s)\n",
      "creating index...\n",
      "index created!\n",
      "None\n",
      "{'classes': ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'), 'palette': [(220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230), (106, 0, 228), (0, 60, 100), (0, 80, 100), (0, 0, 70), (0, 0, 192), (250, 170, 30), (100, 170, 30), (220, 220, 0), (175, 116, 175), (250, 0, 30), (165, 42, 42), (255, 77, 255), (0, 226, 252), (182, 182, 255), (0, 82, 0), (120, 166, 157), (110, 76, 0), (174, 57, 255), (199, 100, 0), (72, 0, 118), (255, 179, 240), (0, 125, 92), (209, 0, 151), (188, 208, 182), (0, 220, 176), (255, 99, 164), (92, 0, 73), (133, 129, 255), (78, 180, 255), (0, 228, 0), (174, 255, 243), (45, 89, 255), (134, 134, 103), (145, 148, 174), (255, 208, 186), (197, 226, 255), (171, 134, 1), (109, 63, 54), (207, 138, 255), (151, 0, 95), (9, 80, 61), (84, 105, 51), (74, 65, 105), (166, 196, 102), (208, 195, 210), (255, 109, 65), (0, 143, 149), (179, 0, 194), (209, 99, 106), (5, 121, 0), (227, 255, 205), (147, 186, 208), (153, 69, 1), (3, 95, 161), (163, 255, 0), (119, 0, 170), (0, 182, 199), (0, 165, 120), (183, 130, 88), (95, 32, 0), (130, 114, 135), (110, 129, 133), (166, 74, 118), (219, 142, 185), (79, 210, 114), (178, 90, 62), (65, 70, 15), (127, 167, 115), (59, 105, 106), (142, 108, 45), (196, 172, 0), (95, 54, 80), (128, 76, 255), (201, 57, 1), (246, 0, 122), (191, 162, 208)]}\n",
      "loading annotations into memory...\n",
      "Done (t=0.33s)\n",
      "creating index...\n",
      "index created!\n",
      "None\n",
      "{'classes': ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'), 'palette': [(220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230), (106, 0, 228), (0, 60, 100), (0, 80, 100), (0, 0, 70), (0, 0, 192), (250, 170, 30), (100, 170, 30), (220, 220, 0), (175, 116, 175), (250, 0, 30), (165, 42, 42), (255, 77, 255), (0, 226, 252), (182, 182, 255), (0, 82, 0), (120, 166, 157), (110, 76, 0), (174, 57, 255), (199, 100, 0), (72, 0, 118), (255, 179, 240), (0, 125, 92), (209, 0, 151), (188, 208, 182), (0, 220, 176), (255, 99, 164), (92, 0, 73), (133, 129, 255), (78, 180, 255), (0, 228, 0), (174, 255, 243), (45, 89, 255), (134, 134, 103), (145, 148, 174), (255, 208, 186), (197, 226, 255), (171, 134, 1), (109, 63, 54), (207, 138, 255), (151, 0, 95), (9, 80, 61), (84, 105, 51), (74, 65, 105), (166, 196, 102), (208, 195, 210), (255, 109, 65), (0, 143, 149), (179, 0, 194), (209, 99, 106), (5, 121, 0), (227, 255, 205), (147, 186, 208), (153, 69, 1), (3, 95, 161), (163, 255, 0), (119, 0, 170), (0, 182, 199), (0, 165, 120), (183, 130, 88), (95, 32, 0), (130, 114, 135), (110, 129, 133), (166, 74, 118), (219, 142, 185), (79, 210, 114), (178, 90, 62), (65, 70, 15), (127, 167, 115), (59, 105, 106), (142, 108, 45), (196, 172, 0), (95, 54, 80), (128, 76, 255), (201, 57, 1), (246, 0, 122), (191, 162, 208)]}\n"
     ]
    }
   ],
   "source": [
    "cfg_tr['val_evaluator']['ann_file'] = cfg_tr['val_evaluator']['ann_file'].replace('data','../data') if '..' not in cfg_tr['val_evaluator']['ann_file'] else cfg_tr['val_evaluator']['ann_file']\n",
    "val_eval = Runner.build_evaluator(_,cfg_tr['val_evaluator'])\n",
    "print(val_eval.dataset_meta)\n",
    "setattr(val_eval,'dataset_meta',val_dataloader.dataset.metainfo)\n",
    "print(val_eval.dataset_meta)\n",
    "# test_evaluator_cfg = deepcopy(cfg_tr['test_evaluator'])\n",
    "# print(train_dataset_cfg['data_root'],cfg_tr['test_dataloader']['dataset']['ann_file'] )\n",
    "# test_evaluator_cfg['ann_file'] = os.path.join(cfg_tr['test_dataloader']['dataset']['data_root'], test_evaluator_cfg['ann_file'])\n",
    "# print(test_evaluator_cfg)\n",
    "cfg_tr['test_evaluator']['ann_file'] = cfg_tr['test_evaluator']['ann_file'].replace('data','../data') if '..' not in cfg_tr['test_evaluator']['ann_file'] else cfg_tr['test_evaluator']['ann_file']\n",
    "test_eval = Runner.build_evaluator(_,cfg_tr['test_evaluator'])\n",
    "print(test_eval.dataset_meta)\n",
    "setattr(test_eval,'dataset_meta',test_dataloader.dataset.metainfo)\n",
    "print(test_eval.dataset_meta)\n",
    "# print(val_eval.dataset_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.\n",
      "  warnings.warn(f'position encoding of key is'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/5000\n",
      "200/5000\n",
      "300/5000\n",
      "400/5000\n",
      "500/5000\n",
      "600/5000\n",
      "700/5000\n",
      "800/5000\n",
      "900/5000\n",
      "1000/5000\n",
      "1100/5000\n",
      "1200/5000\n",
      "1300/5000\n",
      "1400/5000\n",
      "1500/5000\n",
      "1600/5000\n",
      "1700/5000\n",
      "1800/5000\n",
      "1900/5000\n",
      "2000/5000\n",
      "2100/5000\n",
      "2200/5000\n",
      "2300/5000\n",
      "2400/5000\n",
      "2500/5000\n",
      "2600/5000\n",
      "2700/5000\n",
      "2800/5000\n",
      "2900/5000\n",
      "3000/5000\n",
      "3100/5000\n",
      "3200/5000\n",
      "3300/5000\n",
      "3400/5000\n",
      "3500/5000\n",
      "3600/5000\n",
      "3700/5000\n",
      "3800/5000\n",
      "3900/5000\n",
      "4000/5000\n",
      "4100/5000\n",
      "4200/5000\n",
      "4300/5000\n",
      "4400/5000\n",
      "4500/5000\n",
      "4600/5000\n",
      "4700/5000\n",
      "4800/5000\n",
      "4900/5000\n",
      "5000/5000\n",
      "07/24 14:25:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=1.77s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=44.35s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=19.92s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.667\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.552\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.375\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.533\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.742\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.745\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.745\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.604\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.784\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.889\n",
      "07/24 14:26:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.504 0.667 0.552 0.375 0.533 0.650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coco/bbox_mAP': 0.504,\n",
       " 'coco/bbox_mAP_50': 0.667,\n",
       " 'coco/bbox_mAP_75': 0.552,\n",
       " 'coco/bbox_mAP_s': 0.375,\n",
       " 'coco/bbox_mAP_m': 0.533,\n",
       " 'coco/bbox_mAP_l': 0.65}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_init_2 = model_init_2.to(device)\n",
    "model_init_2.eval()\n",
    "for i,data in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        data = to_device(data, device)\n",
    "        data = model_init_2.data_preprocessor(data)\n",
    "        # print(data['inputs'].shape)\n",
    "        # x = torch.tensor(data['inputs'][0], device = device)\n",
    "        # samples = data['data_samples']\n",
    "\n",
    "        out = model_init_2.predict(data['inputs'], data['data_samples'])\n",
    "        test_eval.process(out)\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'{i+1}/{len(test_dataloader)}')\n",
    "\n",
    "test_eval.evaluate(len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfm_task_models.split_models import SplitGDINO\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "split_model = SplitGDINO.create_from_cfg_and_checkpoint(\"../configs/multimodal/mm_grounding_dino_coco.py\", '../checkpoints/multimodal/mm_grounding_dino.pth', device = device, cut_point = 2)\n",
    "# split_model_2 = SplitGDINO.create_from_cfg_and_checkpoint(\"../configs/multimodal/mm_grounding_dino_coco.py\", '../checkpoints/multimodal/mm_grounding_dino.pth', device = device, cut_point = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_model = split_model.to(device)\n",
    "split_model.eval()\n",
    "# split_model_2 = split_model_2.to(device)\n",
    "# split_model_2.eval()\n",
    "model_init = model_init.to(device)\n",
    "# split_model.cut_point = 1\n",
    "print(split_model.data_preprocessor)\n",
    "for i,data in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        data = to_device(data, device)\n",
    "        feats = split_model.feature_frontend(data)\n",
    "        # print(feats)\n",
    "        assert len(feats['inputs'])== len(data['inputs']), f'length of feats is {len(feats[\"inputs\"])} != length of data[\"inputs\"] which is {len(data[\"inputs\"])}'\n",
    "        out = split_model.backend_inference(feats)\n",
    "        \n",
    "        test_eval.process(out)\n",
    "        # break\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'{i+1}/{len(test_dataloader)}')\n",
    "\n",
    "test_eval.evaluate(len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwoInputIdentity()\n",
      "100/5000\n",
      "200/5000\n",
      "300/5000\n",
      "400/5000\n",
      "500/5000\n",
      "600/5000\n",
      "700/5000\n",
      "800/5000\n",
      "900/5000\n",
      "1000/5000\n",
      "1100/5000\n",
      "1200/5000\n",
      "1300/5000\n",
      "1400/5000\n",
      "1500/5000\n",
      "1600/5000\n",
      "1700/5000\n",
      "1800/5000\n",
      "1900/5000\n",
      "2000/5000\n",
      "2100/5000\n",
      "2200/5000\n",
      "2300/5000\n",
      "2400/5000\n",
      "2500/5000\n",
      "2600/5000\n",
      "2700/5000\n",
      "2800/5000\n",
      "2900/5000\n",
      "3000/5000\n",
      "3100/5000\n",
      "3200/5000\n",
      "3300/5000\n",
      "3400/5000\n",
      "3500/5000\n",
      "3600/5000\n",
      "3700/5000\n",
      "3800/5000\n",
      "3900/5000\n",
      "4000/5000\n",
      "4100/5000\n",
      "4200/5000\n",
      "4300/5000\n",
      "4400/5000\n",
      "4500/5000\n",
      "4600/5000\n",
      "4700/5000\n",
      "4800/5000\n",
      "4900/5000\n",
      "5000/5000\n",
      "07/26 15:20:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=1.91s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=45.20s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=19.56s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.667\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.552\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.375\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.533\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.742\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.745\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.745\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.604\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.784\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.889\n",
      "07/26 15:22:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.504 0.667 0.552 0.375 0.533 0.650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coco/bbox_mAP': 0.504,\n",
       " 'coco/bbox_mAP_50': 0.667,\n",
       " 'coco/bbox_mAP_75': 0.552,\n",
       " 'coco/bbox_mAP_s': 0.375,\n",
       " 'coco/bbox_mAP_m': 0.533,\n",
       " 'coco/bbox_mAP_l': 0.65}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_model = split_model.to(device)\n",
    "split_model.eval()\n",
    "# split_model_2 = split_model_2.to(device)\n",
    "# split_model_2.eval()\n",
    "model_init = model_init.to(device)\n",
    "# split_model.cut_point = 1\n",
    "print(split_model.data_preprocessor)\n",
    "for i,data in enumerate(val_dataloader):\n",
    "    with torch.no_grad():\n",
    "        data = to_device(data, device)\n",
    "        feats = split_model.feature_frontend(data)\n",
    "        assert len(feats['inputs'])== len(data['inputs']), f'length of feats is {len(feats[\"inputs\"])} != length of data[\"inputs\"] which is {len(data[\"inputs\"])}'\n",
    "\n",
    "        # loss = split_model.backend_loss(feats)\n",
    "        out = split_model.backend_inference(feats)\n",
    "    # break\n",
    "        test_eval.process(out)\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'{i+1}/{len(test_dataloader)}')\n",
    "\n",
    "test_eval.evaluate(len(test_dataloader.dataset))\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-17 16:02:53--  https://download.openmmlab.com/mmdetection/v3.0/glip/glip_tiny_mmdet-c24ce662.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 8.25.82.176, 8.25.82.179, 8.25.82.177, ...\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|8.25.82.176|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 927536362 (885M) [application/octet-stream]\n",
      "Saving to: ‘../checkpoints/multimodal/glip.pth’\n",
      "\n",
      "                ../   3%[                    ]  29.43M   492KB/s    eta 4h 30m ^C\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://download.openmmlab.com/mmdetection/v3.0/glip/glip_tiny_mmdet-c24ce662.pth' -O ../checkpoints/multimodal/glip.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ../checkpoints/multimodal/glip.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: language_model.language_backbone.body.model.embeddings.position_ids\n",
      "\n"
     ]
    }
   ],
   "source": [
    "glip_model = init_detector('../configs/multimodal/glip_t.py','../checkpoints/multimodal/glip.pth', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/5000\n",
      "200/5000\n",
      "300/5000\n",
      "400/5000\n",
      "500/5000\n",
      "600/5000\n",
      "700/5000\n",
      "800/5000\n",
      "900/5000\n",
      "1000/5000\n",
      "1100/5000\n",
      "1200/5000\n",
      "1300/5000\n",
      "1400/5000\n",
      "1500/5000\n",
      "1600/5000\n",
      "1700/5000\n",
      "1800/5000\n",
      "1900/5000\n",
      "2000/5000\n",
      "2100/5000\n",
      "2200/5000\n",
      "2300/5000\n",
      "2400/5000\n",
      "2500/5000\n",
      "2600/5000\n",
      "2700/5000\n",
      "2800/5000\n",
      "2900/5000\n",
      "3000/5000\n",
      "3100/5000\n",
      "3200/5000\n",
      "3300/5000\n",
      "3400/5000\n",
      "3500/5000\n",
      "3600/5000\n",
      "3700/5000\n",
      "3800/5000\n",
      "3900/5000\n",
      "4000/5000\n",
      "4100/5000\n",
      "4200/5000\n",
      "4300/5000\n",
      "4400/5000\n",
      "4500/5000\n",
      "4600/5000\n",
      "4700/5000\n",
      "4800/5000\n",
      "4900/5000\n",
      "5000/5000\n",
      "07/18 11:55:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=17.87s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.67s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.552\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.735\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.606\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.409\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.599\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.691\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.714\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.714\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.714\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.566\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.764\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.847\n",
      "07/18 11:55:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.552 0.735 0.606 0.409 0.599 0.691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coco/bbox_mAP': 0.552,\n",
       " 'coco/bbox_mAP_50': 0.735,\n",
       " 'coco/bbox_mAP_75': 0.606,\n",
       " 'coco/bbox_mAP_s': 0.409,\n",
       " 'coco/bbox_mAP_m': 0.599,\n",
       " 'coco/bbox_mAP_l': 0.691}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "glip_model = glip_model.to(device)\n",
    "glip_model.eval()\n",
    "for i,data in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        data = to_device(data, device)\n",
    "        data = glip_model.data_preprocessor(data)\n",
    "        # print(data['inputs'].shape)\n",
    "        # x = torch.tensor(data['inputs'][0], device = device)\n",
    "        # samples = data['data_samples']\n",
    "\n",
    "        out = glip_model.predict(data['inputs'], data['data_samples'])\n",
    "        test_eval.process(out)\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'{i+1}/{len(test_dataloader)}')\n",
    "\n",
    "test_eval.evaluate(len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ../checkpoints/multimodal/glip.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: language_model.language_backbone.body.model.embeddings.position_ids\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cfm_task_models.split_models import SplitGLIP\n",
    "glip_split = SplitGLIP.create_from_cfg_and_checkpoint(\"../configs/multimodal/glip_t.py\", '../checkpoints/multimodal/glip.pth', device = device, cut_point = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwoInputIdentity()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1060: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/torch/nn/functional.py:4070: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/5000\n",
      "200/5000\n",
      "300/5000\n",
      "400/5000\n",
      "500/5000\n",
      "600/5000\n",
      "700/5000\n",
      "800/5000\n",
      "900/5000\n",
      "1000/5000\n",
      "1100/5000\n",
      "1200/5000\n",
      "1300/5000\n",
      "1400/5000\n",
      "1500/5000\n",
      "1600/5000\n",
      "1700/5000\n",
      "1800/5000\n",
      "1900/5000\n",
      "2000/5000\n",
      "2100/5000\n",
      "2200/5000\n",
      "2300/5000\n",
      "2400/5000\n",
      "2500/5000\n",
      "2600/5000\n",
      "2700/5000\n",
      "2800/5000\n",
      "2900/5000\n",
      "3000/5000\n",
      "3100/5000\n",
      "3200/5000\n",
      "3300/5000\n",
      "3400/5000\n",
      "3500/5000\n",
      "3600/5000\n",
      "3700/5000\n",
      "3800/5000\n",
      "3900/5000\n",
      "4000/5000\n",
      "4100/5000\n",
      "4200/5000\n",
      "4300/5000\n",
      "4400/5000\n",
      "4500/5000\n",
      "4600/5000\n",
      "4700/5000\n",
      "4800/5000\n",
      "4900/5000\n",
      "5000/5000\n",
      "07/18 12:21:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.58s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=18.21s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.73s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.552\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.735\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.606\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.409\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.599\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.691\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.714\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.714\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.714\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.566\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.764\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.847\n",
      "07/18 12:22:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.552 0.735 0.606 0.409 0.599 0.691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coco/bbox_mAP': 0.552,\n",
       " 'coco/bbox_mAP_50': 0.735,\n",
       " 'coco/bbox_mAP_75': 0.606,\n",
       " 'coco/bbox_mAP_s': 0.409,\n",
       " 'coco/bbox_mAP_m': 0.599,\n",
       " 'coco/bbox_mAP_l': 0.691}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "glip_split = glip_split.to(device)\n",
    "glip_split.eval()\n",
    "# split_model_2 = split_model_2.to(device)\n",
    "# split_model_2.eval()\n",
    "# split_model.cut_point = 1\n",
    "print(glip_split.data_preprocessor)\n",
    "for i,data in enumerate(val_dataloader):\n",
    "    # with torch.no_grad():\n",
    "    data = to_device(data, device)\n",
    "    feats = glip_split.feature_frontend(data)\n",
    "    # assert len(feats['inputs'])== len(data['inputs']), f'length of feats is {len(feats[\"inputs\"])} != length of data[\"inputs\"] which is {len(data[\"inputs\"])}'\n",
    "    \n",
    "    # loss = split_model.backend_loss(feats)\n",
    "    out = glip_split.backend_inference(feats)\n",
    "    # break\n",
    "    test_eval.process(out)\n",
    "    if (i+1)%100 == 0:\n",
    "        print(f'{i+1}/{len(test_dataloader)}')\n",
    "\n",
    "test_eval.evaluate(len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Loads checkpoint by local backend from path: ../checkpoints/detection/mask2former_swin_instance.pth\n",
      "{'type': 'CocoDataset', 'data_root': '../data/coco/', 'ann_file': 'annotations/instances_val2017.json', 'data_prefix': {'img': 'val2017/', 'seg': 'annotations/panoptic_val2017/'}, 'test_mode': True, 'pipeline': [{'type': 'LoadImageFromFile', 'to_float32': True, 'backend_args': None}, {'type': 'Resize', 'scale': (1333, 800), 'keep_ratio': True}, {'type': 'LoadAnnotations', 'with_bbox': True, 'with_mask': True}, {'type': 'PackDetInputs', 'meta_keys': ('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor')}], 'backend_args': None}\n",
      "annotations/instances_val2017.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.20s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.19s)\n",
      "creating index...\n",
      "index created!\n",
      "5000\n",
      "loading annotations into memory...\n",
      "Done (t=0.19s)\n",
      "creating index...\n",
      "index created!\n",
      "None\n",
      "{'classes': ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'), 'palette': [(220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230), (106, 0, 228), (0, 60, 100), (0, 80, 100), (0, 0, 70), (0, 0, 192), (250, 170, 30), (100, 170, 30), (220, 220, 0), (175, 116, 175), (250, 0, 30), (165, 42, 42), (255, 77, 255), (0, 226, 252), (182, 182, 255), (0, 82, 0), (120, 166, 157), (110, 76, 0), (174, 57, 255), (199, 100, 0), (72, 0, 118), (255, 179, 240), (0, 125, 92), (209, 0, 151), (188, 208, 182), (0, 220, 176), (255, 99, 164), (92, 0, 73), (133, 129, 255), (78, 180, 255), (0, 228, 0), (174, 255, 243), (45, 89, 255), (134, 134, 103), (145, 148, 174), (255, 208, 186), (197, 226, 255), (171, 134, 1), (109, 63, 54), (207, 138, 255), (151, 0, 95), (9, 80, 61), (84, 105, 51), (74, 65, 105), (166, 196, 102), (208, 195, 210), (255, 109, 65), (0, 143, 149), (179, 0, 194), (209, 99, 106), (5, 121, 0), (227, 255, 205), (147, 186, 208), (153, 69, 1), (3, 95, 161), (163, 255, 0), (119, 0, 170), (0, 182, 199), (0, 165, 120), (183, 130, 88), (95, 32, 0), (130, 114, 135), (110, 129, 133), (166, 74, 118), (219, 142, 185), (79, 210, 114), (178, 90, 62), (65, 70, 15), (127, 167, 115), (59, 105, 106), (142, 108, 45), (196, 172, 0), (95, 54, 80), (128, 76, 255), (201, 57, 1), (246, 0, 122), (191, 162, 208)]}\n",
      "loading annotations into memory...\n",
      "Done (t=0.33s)\n",
      "creating index...\n",
      "index created!\n",
      "None\n",
      "{'classes': ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'), 'palette': [(220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230), (106, 0, 228), (0, 60, 100), (0, 80, 100), (0, 0, 70), (0, 0, 192), (250, 170, 30), (100, 170, 30), (220, 220, 0), (175, 116, 175), (250, 0, 30), (165, 42, 42), (255, 77, 255), (0, 226, 252), (182, 182, 255), (0, 82, 0), (120, 166, 157), (110, 76, 0), (174, 57, 255), (199, 100, 0), (72, 0, 118), (255, 179, 240), (0, 125, 92), (209, 0, 151), (188, 208, 182), (0, 220, 176), (255, 99, 164), (92, 0, 73), (133, 129, 255), (78, 180, 255), (0, 228, 0), (174, 255, 243), (45, 89, 255), (134, 134, 103), (145, 148, 174), (255, 208, 186), (197, 226, 255), (171, 134, 1), (109, 63, 54), (207, 138, 255), (151, 0, 95), (9, 80, 61), (84, 105, 51), (74, 65, 105), (166, 196, 102), (208, 195, 210), (255, 109, 65), (0, 143, 149), (179, 0, 194), (209, 99, 106), (5, 121, 0), (227, 255, 205), (147, 186, 208), (153, 69, 1), (3, 95, 161), (163, 255, 0), (119, 0, 170), (0, 182, 199), (0, 165, 120), (183, 130, 88), (95, 32, 0), (130, 114, 135), (110, 129, 133), (166, 74, 118), (219, 142, 185), (79, 210, 114), (178, 90, 62), (65, 70, 15), (127, 167, 115), (59, 105, 106), (142, 108, 45), (196, 172, 0), (95, 54, 80), (128, 76, 255), (201, 57, 1), (246, 0, 122), (191, 162, 208)]}\n"
     ]
    }
   ],
   "source": [
    "from mmdet.models import Mask2Former\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mmdet.apis import init_detector\n",
    "import torch\n",
    "mask2former_swin = init_detector('../configs/detection/mask2former_swin_t.py','../checkpoints/detection/mask2former_swin_instance.pth', device='cpu')\n",
    "\n",
    "# Mask2former needs separate dataloaders and evaluators because it needs masks etc.\n",
    "\n",
    "cfg_tr = mask2former_swin.cfg.copy()\n",
    "cfg_tr['val_dataloader']['dataset']['data_root'] = '../data/coco/'\n",
    "cfg_tr['test_dataloader']['dataset']['data_root'] = '../data/coco/'\n",
    "print(cfg_tr['test_dataloader']['dataset'])\n",
    "# cfg_tr['train_dataloader']['batch_size'] = 8\n",
    "# cfg_tr['train_dataloader']['num_workers'] = 1\n",
    "\n",
    "# train_dataloader = Runner.build_dataloader(cfg_tr['train_dataloader']) \n",
    "\n",
    "print(cfg_tr['val_dataloader']['dataset']['ann_file'])\n",
    "val_dataloader = Runner.build_dataloader(cfg_tr['val_dataloader'])\n",
    "\n",
    "test_dataloader = Runner.build_dataloader(cfg_tr['test_dataloader'])\n",
    "print(len(val_dataloader.dataset))\n",
    "\n",
    "cfg_tr['val_evaluator']['ann_file'] = cfg_tr['val_evaluator']['ann_file'].replace('data','../data') if '..' not in cfg_tr['val_evaluator']['ann_file'] else cfg_tr['val_evaluator']['ann_file']\n",
    "val_eval = Runner.build_evaluator(_,cfg_tr['val_evaluator'])\n",
    "print(val_eval.dataset_meta)\n",
    "setattr(val_eval,'dataset_meta',val_dataloader.dataset.metainfo)\n",
    "print(val_eval.dataset_meta)\n",
    "# test_evaluator_cfg = deepcopy(cfg_tr['test_evaluator'])\n",
    "# print(train_dataset_cfg['data_root'],cfg_tr['test_dataloader']['dataset']['ann_file'] )\n",
    "# test_evaluator_cfg['ann_file'] = os.path.join(cfg_tr['test_dataloader']['dataset']['data_root'], test_evaluator_cfg['ann_file'])\n",
    "# print(test_evaluator_cfg)\n",
    "cfg_tr['test_evaluator']['ann_file'] = cfg_tr['test_evaluator']['ann_file'].replace('data','../data') if '..' not in cfg_tr['test_evaluator']['ann_file'] else cfg_tr['test_evaluator']['ann_file']\n",
    "test_eval = Runner.build_evaluator(_,cfg_tr['test_evaluator'])\n",
    "print(test_eval.dataset_meta)\n",
    "setattr(test_eval,'dataset_meta',test_dataloader.dataset.metainfo)\n",
    "print(test_eval.dataset_meta)\n",
    "# print(val_eval.dataset_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/5000\n",
      "200/5000\n",
      "300/5000\n",
      "400/5000\n",
      "500/5000\n",
      "600/5000\n",
      "700/5000\n",
      "800/5000\n",
      "900/5000\n",
      "1000/5000\n",
      "1100/5000\n",
      "1200/5000\n",
      "1300/5000\n",
      "1400/5000\n",
      "1500/5000\n",
      "1600/5000\n",
      "1700/5000\n",
      "1800/5000\n",
      "1900/5000\n",
      "2000/5000\n",
      "2100/5000\n",
      "2200/5000\n",
      "2300/5000\n",
      "2400/5000\n",
      "2500/5000\n",
      "2600/5000\n",
      "2700/5000\n",
      "2800/5000\n",
      "2900/5000\n",
      "3000/5000\n",
      "3100/5000\n",
      "3200/5000\n",
      "3300/5000\n",
      "3400/5000\n",
      "3500/5000\n",
      "3600/5000\n",
      "3700/5000\n",
      "3800/5000\n",
      "3900/5000\n",
      "4000/5000\n",
      "4100/5000\n",
      "4200/5000\n",
      "4300/5000\n",
      "4400/5000\n",
      "4500/5000\n",
      "4600/5000\n",
      "4700/5000\n",
      "4800/5000\n",
      "4900/5000\n",
      "5000/5000\n",
      "07/19 15:35:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.74s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=21.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=6.92s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.477\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.676\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.511\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.294\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.506\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.654\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.457\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.684\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.841\n",
      "07/19 15:35:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.477 0.676 0.511 0.294 0.506 0.654\n",
      "07/19 15:35:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating segm...\n",
      "Loading and preparing results...\n",
      "DONE (t=2.62s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=25.16s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=6.88s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.447\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.679\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.481\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.241\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.481\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.670\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.592\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.592\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.592\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.627\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.788\n",
      "07/19 15:36:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - segm_mAP_copypaste: 0.447 0.679 0.481 0.241 0.481 0.670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coco/bbox_mAP': 0.477,\n",
       " 'coco/bbox_mAP_50': 0.676,\n",
       " 'coco/bbox_mAP_75': 0.511,\n",
       " 'coco/bbox_mAP_s': 0.294,\n",
       " 'coco/bbox_mAP_m': 0.506,\n",
       " 'coco/bbox_mAP_l': 0.654,\n",
       " 'coco/segm_mAP': 0.447,\n",
       " 'coco/segm_mAP_50': 0.679,\n",
       " 'coco/segm_mAP_75': 0.481,\n",
       " 'coco/segm_mAP_s': 0.241,\n",
       " 'coco/segm_mAP_m': 0.481,\n",
       " 'coco/segm_mAP_l': 0.67}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "mask2former_swin = mask2former_swin.to(device)\n",
    "mask2former_swin.eval()\n",
    "for i,data in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        data = to_device(data, device)\n",
    "        data = mask2former_swin.data_preprocessor(data)\n",
    "        # print(data['inputs'].shape)\n",
    "        # x = torch.tensor(data['inputs'][0], device = device)\n",
    "        # samples = data['data_samples']\n",
    "\n",
    "        out = mask2former_swin.predict(data['inputs'], data['data_samples'])\n",
    "        test_eval.process(out)\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'{i+1}/{len(test_dataloader)}')\n",
    "\n",
    "test_eval.evaluate(len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ../checkpoints/detection/mask2former_instance.pth\n"
     ]
    }
   ],
   "source": [
    "from cfm_task_models.split_models import SplitMask2Former\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "mask2former_swin_split = SplitMask2Former.create_from_cfg_and_checkpoint(\"../configs/detection/mask2former_swin_t.py\", '../checkpoints/detection/mask2former_instance.pth', device = device, cut_point = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetDataPreprocessor(\n",
      "  (batch_augments): ModuleList(\n",
      "    (0): BatchFixedSizePad()\n",
      "  )\n",
      ")\n",
      "DetDataPreprocessor(\n",
      "  (batch_augments): ModuleList(\n",
      "    (0): BatchFixedSizePad()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mask2former_swin_split.frontend_preprocessor)\n",
    "print(mask2former_swin.data_preprocessor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetDataPreprocessor(\n",
      "  (batch_augments): ModuleList(\n",
      "    (0): BatchFixedSizePad()\n",
      "  )\n",
      ")\n",
      "100/5000\n",
      "200/5000\n",
      "300/5000\n",
      "400/5000\n",
      "500/5000\n",
      "600/5000\n",
      "700/5000\n",
      "800/5000\n",
      "900/5000\n",
      "1000/5000\n",
      "1100/5000\n",
      "1200/5000\n",
      "1300/5000\n",
      "1400/5000\n",
      "1500/5000\n",
      "1600/5000\n",
      "1700/5000\n",
      "1800/5000\n",
      "1900/5000\n",
      "2000/5000\n",
      "2100/5000\n",
      "2200/5000\n",
      "2300/5000\n",
      "2400/5000\n",
      "2500/5000\n",
      "2600/5000\n",
      "2700/5000\n",
      "2800/5000\n",
      "2900/5000\n",
      "3000/5000\n",
      "3100/5000\n",
      "3200/5000\n",
      "3300/5000\n",
      "3400/5000\n",
      "3500/5000\n",
      "3600/5000\n",
      "3700/5000\n",
      "3800/5000\n",
      "3900/5000\n",
      "4000/5000\n",
      "4100/5000\n",
      "4200/5000\n",
      "4300/5000\n",
      "4400/5000\n",
      "4500/5000\n",
      "4600/5000\n",
      "4700/5000\n",
      "4800/5000\n",
      "4900/5000\n",
      "5000/5000\n",
      "07/24 09:57:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.69s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=20.81s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=6.99s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.477\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.676\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.511\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.294\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.506\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.654\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.457\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.684\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.841\n",
      "07/24 09:57:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.477 0.676 0.511 0.294 0.506 0.654\n",
      "07/24 09:57:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating segm...\n",
      "Loading and preparing results...\n",
      "DONE (t=2.51s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=24.32s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=7.88s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.447\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.679\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.481\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.241\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.481\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.670\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.592\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.592\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.592\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.627\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.788\n",
      "07/24 09:58:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - segm_mAP_copypaste: 0.447 0.679 0.481 0.241 0.481 0.670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coco/bbox_mAP': 0.477,\n",
       " 'coco/bbox_mAP_50': 0.676,\n",
       " 'coco/bbox_mAP_75': 0.511,\n",
       " 'coco/bbox_mAP_s': 0.294,\n",
       " 'coco/bbox_mAP_m': 0.506,\n",
       " 'coco/bbox_mAP_l': 0.654,\n",
       " 'coco/segm_mAP': 0.447,\n",
       " 'coco/segm_mAP_50': 0.679,\n",
       " 'coco/segm_mAP_75': 0.481,\n",
       " 'coco/segm_mAP_s': 0.241,\n",
       " 'coco/segm_mAP_m': 0.481,\n",
       " 'coco/segm_mAP_l': 0.67}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cfm_task_models.split_utils import SplitSwinFeatures\n",
    "mask2former_swin_split = mask2former_swin_split.to(device)\n",
    "mask2former_swin_split.eval()\n",
    "# split_model_2 = split_model_2.to(device)\n",
    "# split_model_2.eval()\n",
    "# split_model.cut_point = 1\n",
    "print(mask2former_swin_split.frontend_preprocessor)\n",
    "for i,data in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "    # break\n",
    "    # data = to_device(data, device)\n",
    "    # print(data)\n",
    "    # data = mask2former_swin.data_preprocessor(data)\n",
    "    # print(data)\n",
    "        # feats = mask2former_swin_split.frontend_preprocessor(data)\n",
    "        # print(feats)\n",
    "        # out =  mask2former_swin_split.backbone.split_forward_v2(feats['inputs'], output_layer = mask2former_swin_split.cut_point-1)\n",
    "        #     # data['inputs'] = {\"hw_shape\": out[0],\n",
    "        #     #                   \"outs\": out[1]}\n",
    "        # feats['inputs'] = SplitSwinFeatures(outs=out[1], hw_shape=out[0], device=device)\n",
    "        feats = mask2former_swin_split.feature_frontend(data)\n",
    "        # assert len(feats['inputs'])== len(data['inputs']), f'length of feats is {len(feats[\"inputs\"])} != length of data[\"inputs\"] which is {len(data[\"inputs\"])}'\n",
    "        \n",
    "        # loss = split_model.backend_loss(feats)\n",
    "        out = mask2former_swin_split.backend_inference(feats)\n",
    "        # break\n",
    "        test_eval.process(out)\n",
    "        # break\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'{i+1}/{len(test_dataloader)}')\n",
    "\n",
    "test_eval.evaluate(len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Loads checkpoint by local backend from path: ../checkpoints/detection/mask2former_r50.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'CocoDataset', 'data_root': '../data/coco/', 'ann_file': 'annotations/instances_val2017.json', 'data_prefix': {'img': 'val2017/', 'seg': 'annotations/panoptic_val2017/'}, 'test_mode': True, 'pipeline': [{'type': 'LoadImageFromFile', 'to_float32': True, 'backend_args': None}, {'type': 'Resize', 'scale': (1333, 800), 'keep_ratio': True}, {'type': 'LoadAnnotations', 'with_bbox': True, 'with_mask': True}, {'type': 'PackDetInputs', 'meta_keys': ('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor')}], 'backend_args': None}\n",
      "annotations/instances_val2017.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.27s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.27s)\n",
      "creating index...\n",
      "index created!\n",
      "5000\n",
      "loading annotations into memory...\n",
      "Done (t=0.26s)\n",
      "creating index...\n",
      "index created!\n",
      "None\n",
      "{'classes': ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'), 'palette': [(220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230), (106, 0, 228), (0, 60, 100), (0, 80, 100), (0, 0, 70), (0, 0, 192), (250, 170, 30), (100, 170, 30), (220, 220, 0), (175, 116, 175), (250, 0, 30), (165, 42, 42), (255, 77, 255), (0, 226, 252), (182, 182, 255), (0, 82, 0), (120, 166, 157), (110, 76, 0), (174, 57, 255), (199, 100, 0), (72, 0, 118), (255, 179, 240), (0, 125, 92), (209, 0, 151), (188, 208, 182), (0, 220, 176), (255, 99, 164), (92, 0, 73), (133, 129, 255), (78, 180, 255), (0, 228, 0), (174, 255, 243), (45, 89, 255), (134, 134, 103), (145, 148, 174), (255, 208, 186), (197, 226, 255), (171, 134, 1), (109, 63, 54), (207, 138, 255), (151, 0, 95), (9, 80, 61), (84, 105, 51), (74, 65, 105), (166, 196, 102), (208, 195, 210), (255, 109, 65), (0, 143, 149), (179, 0, 194), (209, 99, 106), (5, 121, 0), (227, 255, 205), (147, 186, 208), (153, 69, 1), (3, 95, 161), (163, 255, 0), (119, 0, 170), (0, 182, 199), (0, 165, 120), (183, 130, 88), (95, 32, 0), (130, 114, 135), (110, 129, 133), (166, 74, 118), (219, 142, 185), (79, 210, 114), (178, 90, 62), (65, 70, 15), (127, 167, 115), (59, 105, 106), (142, 108, 45), (196, 172, 0), (95, 54, 80), (128, 76, 255), (201, 57, 1), (246, 0, 122), (191, 162, 208)]}\n",
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n",
      "None\n",
      "{'classes': ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'), 'palette': [(220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230), (106, 0, 228), (0, 60, 100), (0, 80, 100), (0, 0, 70), (0, 0, 192), (250, 170, 30), (100, 170, 30), (220, 220, 0), (175, 116, 175), (250, 0, 30), (165, 42, 42), (255, 77, 255), (0, 226, 252), (182, 182, 255), (0, 82, 0), (120, 166, 157), (110, 76, 0), (174, 57, 255), (199, 100, 0), (72, 0, 118), (255, 179, 240), (0, 125, 92), (209, 0, 151), (188, 208, 182), (0, 220, 176), (255, 99, 164), (92, 0, 73), (133, 129, 255), (78, 180, 255), (0, 228, 0), (174, 255, 243), (45, 89, 255), (134, 134, 103), (145, 148, 174), (255, 208, 186), (197, 226, 255), (171, 134, 1), (109, 63, 54), (207, 138, 255), (151, 0, 95), (9, 80, 61), (84, 105, 51), (74, 65, 105), (166, 196, 102), (208, 195, 210), (255, 109, 65), (0, 143, 149), (179, 0, 194), (209, 99, 106), (5, 121, 0), (227, 255, 205), (147, 186, 208), (153, 69, 1), (3, 95, 161), (163, 255, 0), (119, 0, 170), (0, 182, 199), (0, 165, 120), (183, 130, 88), (95, 32, 0), (130, 114, 135), (110, 129, 133), (166, 74, 118), (219, 142, 185), (79, 210, 114), (178, 90, 62), (65, 70, 15), (127, 167, 115), (59, 105, 106), (142, 108, 45), (196, 172, 0), (95, 54, 80), (128, 76, 255), (201, 57, 1), (246, 0, 122), (191, 162, 208)]}\n"
     ]
    }
   ],
   "source": [
    "from mmdet.models import Mask2Former\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mmdet.apis import init_detector\n",
    "import torch\n",
    "mask2former_r50 = init_detector('../configs/detection/mask2former_r50.py','../checkpoints/detection/mask2former_r50.pth', device='cpu')\n",
    "\n",
    "# Mask2former needs separate dataloaders and evaluators because it needs masks etc.\n",
    "\n",
    "cfg_tr = mask2former_r50.cfg.copy()\n",
    "cfg_tr['val_dataloader']['dataset']['data_root'] = '../data/coco/'\n",
    "cfg_tr['test_dataloader']['dataset']['data_root'] = '../data/coco/'\n",
    "print(cfg_tr['test_dataloader']['dataset'])\n",
    "# cfg_tr['train_dataloader']['batch_size'] = 8\n",
    "# cfg_tr['train_dataloader']['num_workers'] = 1\n",
    "\n",
    "# train_dataloader = Runner.build_dataloader(cfg_tr['train_dataloader']) \n",
    "\n",
    "print(cfg_tr['val_dataloader']['dataset']['ann_file'])\n",
    "val_dataloader = Runner.build_dataloader(cfg_tr['val_dataloader'])\n",
    "\n",
    "test_dataloader = Runner.build_dataloader(cfg_tr['test_dataloader'])\n",
    "print(len(val_dataloader.dataset))\n",
    "\n",
    "cfg_tr['val_evaluator']['ann_file'] = cfg_tr['val_evaluator']['ann_file'].replace('data','../data') if '..' not in cfg_tr['val_evaluator']['ann_file'] else cfg_tr['val_evaluator']['ann_file']\n",
    "val_eval = Runner.build_evaluator(_,cfg_tr['val_evaluator'])\n",
    "print(val_eval.dataset_meta)\n",
    "setattr(val_eval,'dataset_meta',val_dataloader.dataset.metainfo)\n",
    "print(val_eval.dataset_meta)\n",
    "# test_evaluator_cfg = deepcopy(cfg_tr['test_evaluator'])\n",
    "# print(train_dataset_cfg['data_root'],cfg_tr['test_dataloader']['dataset']['ann_file'] )\n",
    "# test_evaluator_cfg['ann_file'] = os.path.join(cfg_tr['test_dataloader']['dataset']['data_root'], test_evaluator_cfg['ann_file'])\n",
    "# print(test_evaluator_cfg)\n",
    "cfg_tr['test_evaluator']['ann_file'] = cfg_tr['test_evaluator']['ann_file'].replace('data','../data') if '..' not in cfg_tr['test_evaluator']['ann_file'] else cfg_tr['test_evaluator']['ann_file']\n",
    "test_eval = Runner.build_evaluator(_,cfg_tr['test_evaluator'])\n",
    "print(test_eval.dataset_meta)\n",
    "setattr(test_eval,'dataset_meta',test_dataloader.dataset.metainfo)\n",
    "print(test_eval.dataset_meta)\n",
    "# print(val_eval.dataset_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ../checkpoints/detection/mask2former_r50.pth\n"
     ]
    }
   ],
   "source": [
    "from cfm_task_models.split_models import SplitMask2Former\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "mask2former_r50_split = SplitMask2Former.create_from_cfg_and_checkpoint(\"../configs/detection/mask2former_r50.py\", '../checkpoints/detection/mask2former_r50.pth', device = device, cut_point = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetDataPreprocessor(\n",
      "  (batch_augments): ModuleList(\n",
      "    (0): BatchFixedSizePad()\n",
      "  )\n",
      ")\n",
      "100/5000\n",
      "200/5000\n",
      "300/5000\n",
      "400/5000\n",
      "500/5000\n",
      "600/5000\n",
      "700/5000\n",
      "800/5000\n",
      "900/5000\n",
      "1000/5000\n",
      "1100/5000\n",
      "1200/5000\n",
      "1300/5000\n",
      "1400/5000\n",
      "1500/5000\n",
      "1600/5000\n",
      "1700/5000\n",
      "1800/5000\n",
      "1900/5000\n",
      "2000/5000\n",
      "2100/5000\n",
      "2200/5000\n",
      "2300/5000\n",
      "2400/5000\n",
      "2500/5000\n",
      "2600/5000\n",
      "2700/5000\n",
      "2800/5000\n",
      "2900/5000\n",
      "3000/5000\n",
      "3100/5000\n",
      "3200/5000\n",
      "3300/5000\n",
      "3400/5000\n",
      "3500/5000\n",
      "3600/5000\n",
      "3700/5000\n",
      "3800/5000\n",
      "3900/5000\n",
      "4000/5000\n",
      "4100/5000\n",
      "4200/5000\n",
      "4300/5000\n",
      "4400/5000\n",
      "4500/5000\n",
      "4600/5000\n",
      "4700/5000\n",
      "4800/5000\n",
      "4900/5000\n",
      "5000/5000\n",
      "07/26 14:47:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=1.08s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=20.75s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=6.85s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.457\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.651\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.491\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.268\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.485\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.626\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.637\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.637\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.637\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.424\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.674\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.830\n",
      "07/26 14:48:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.457 0.651 0.491 0.268 0.485 0.626\n",
      "07/26 14:48:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating segm...\n",
      "Loading and preparing results...\n",
      "DONE (t=2.55s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=25.30s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=6.97s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.429\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.652\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.460\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.220\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.464\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.647\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.581\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.581\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.581\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.372\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.619\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.779\n",
      "07/26 14:48:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - segm_mAP_copypaste: 0.429 0.652 0.460 0.220 0.464 0.647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coco/bbox_mAP': 0.457,\n",
       " 'coco/bbox_mAP_50': 0.651,\n",
       " 'coco/bbox_mAP_75': 0.491,\n",
       " 'coco/bbox_mAP_s': 0.268,\n",
       " 'coco/bbox_mAP_m': 0.485,\n",
       " 'coco/bbox_mAP_l': 0.626,\n",
       " 'coco/segm_mAP': 0.429,\n",
       " 'coco/segm_mAP_50': 0.652,\n",
       " 'coco/segm_mAP_75': 0.46,\n",
       " 'coco/segm_mAP_s': 0.22,\n",
       " 'coco/segm_mAP_m': 0.464,\n",
       " 'coco/segm_mAP_l': 0.647}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask2former_r50_split = mask2former_r50_split.to(device)\n",
    "mask2former_r50_split.eval()\n",
    "# split_model_2 = split_model_2.to(device)\n",
    "# split_model_2.eval()\n",
    "# split_model.cut_point = 1\n",
    "print(mask2former_r50_split.frontend_preprocessor)\n",
    "for i,data in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "    # break\n",
    "    # data = to_device(data, device)\n",
    "    # print(data)\n",
    "    # data = mask2former_swin.data_preprocessor(data)\n",
    "    # print(data)\n",
    "        # feats = mask2former_swin_split.frontend_preprocessor(data)\n",
    "        # print(feats)\n",
    "        # out =  mask2former_swin_split.backbone.split_forward_v2(feats['inputs'], output_layer = mask2former_swin_split.cut_point-1)\n",
    "        #     # data['inputs'] = {\"hw_shape\": out[0],\n",
    "        #     #                   \"outs\": out[1]}\n",
    "        # feats['inputs'] = SplitSwinFeatures(outs=out[1], hw_shape=out[0], device=device)\n",
    "        feats = mask2former_r50_split.feature_frontend(data)\n",
    "        # assert len(feats['inputs'])== len(data['inputs']), f'length of feats is {len(feats[\"inputs\"])} != length of data[\"inputs\"] which is {len(data[\"inputs\"])}'\n",
    "        # print([t.shape for t in feats['inputs'].outs])\n",
    "        \n",
    "        # break\n",
    "        # loss = split_model.backend_loss(feats)\n",
    "        # print(len(feats['inputs'].outs))\n",
    "        out = mask2former_r50_split.backend_inference(feats)\n",
    "        # break\n",
    "        test_eval.process(out)\n",
    "        # break\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'{i+1}/{len(test_dataloader)}')\n",
    "\n",
    "test_eval.evaluate(len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ../checkpoints/detection/retinanet_r50.pth\n"
     ]
    }
   ],
   "source": [
    "from cfm_task_models.split_utils import NetworkSplitter\n",
    "from mmdet.models import RetinaNet\n",
    "SplitRetinaNet = NetworkSplitter()(RetinaNet)\n",
    "split_retina = SplitRetinaNet.create_from_cfg_and_checkpoint(\"../configs/detection/retinanet_r50.py\", '../checkpoints/detection/retinanet_r50.pth', device = device, cut_point = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ../checkpoints/detection/retinanet_r50.pth\n",
      "{'type': 'CocoDataset', 'data_root': '../data/coco/', 'ann_file': 'annotations/instances_val2017.json', 'data_prefix': {'img': 'val2017/'}, 'test_mode': True, 'pipeline': [{'type': 'LoadImageFromFile', 'backend_args': None}, {'type': 'Resize', 'scale': (1333, 800), 'keep_ratio': True}, {'type': 'LoadAnnotations', 'with_bbox': True}, {'type': 'PackDetInputs', 'meta_keys': ('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor')}], 'backend_args': None}\n",
      "annotations/instances_val2017.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.28s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.28s)\n",
      "creating index...\n",
      "index created!\n",
      "5000\n",
      "loading annotations into memory...\n",
      "Done (t=0.28s)\n",
      "creating index...\n",
      "index created!\n",
      "None\n",
      "{'classes': ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'), 'palette': [(220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230), (106, 0, 228), (0, 60, 100), (0, 80, 100), (0, 0, 70), (0, 0, 192), (250, 170, 30), (100, 170, 30), (220, 220, 0), (175, 116, 175), (250, 0, 30), (165, 42, 42), (255, 77, 255), (0, 226, 252), (182, 182, 255), (0, 82, 0), (120, 166, 157), (110, 76, 0), (174, 57, 255), (199, 100, 0), (72, 0, 118), (255, 179, 240), (0, 125, 92), (209, 0, 151), (188, 208, 182), (0, 220, 176), (255, 99, 164), (92, 0, 73), (133, 129, 255), (78, 180, 255), (0, 228, 0), (174, 255, 243), (45, 89, 255), (134, 134, 103), (145, 148, 174), (255, 208, 186), (197, 226, 255), (171, 134, 1), (109, 63, 54), (207, 138, 255), (151, 0, 95), (9, 80, 61), (84, 105, 51), (74, 65, 105), (166, 196, 102), (208, 195, 210), (255, 109, 65), (0, 143, 149), (179, 0, 194), (209, 99, 106), (5, 121, 0), (227, 255, 205), (147, 186, 208), (153, 69, 1), (3, 95, 161), (163, 255, 0), (119, 0, 170), (0, 182, 199), (0, 165, 120), (183, 130, 88), (95, 32, 0), (130, 114, 135), (110, 129, 133), (166, 74, 118), (219, 142, 185), (79, 210, 114), (178, 90, 62), (65, 70, 15), (127, 167, 115), (59, 105, 106), (142, 108, 45), (196, 172, 0), (95, 54, 80), (128, 76, 255), (201, 57, 1), (246, 0, 122), (191, 162, 208)]}\n",
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n",
      "None\n",
      "{'classes': ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'), 'palette': [(220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230), (106, 0, 228), (0, 60, 100), (0, 80, 100), (0, 0, 70), (0, 0, 192), (250, 170, 30), (100, 170, 30), (220, 220, 0), (175, 116, 175), (250, 0, 30), (165, 42, 42), (255, 77, 255), (0, 226, 252), (182, 182, 255), (0, 82, 0), (120, 166, 157), (110, 76, 0), (174, 57, 255), (199, 100, 0), (72, 0, 118), (255, 179, 240), (0, 125, 92), (209, 0, 151), (188, 208, 182), (0, 220, 176), (255, 99, 164), (92, 0, 73), (133, 129, 255), (78, 180, 255), (0, 228, 0), (174, 255, 243), (45, 89, 255), (134, 134, 103), (145, 148, 174), (255, 208, 186), (197, 226, 255), (171, 134, 1), (109, 63, 54), (207, 138, 255), (151, 0, 95), (9, 80, 61), (84, 105, 51), (74, 65, 105), (166, 196, 102), (208, 195, 210), (255, 109, 65), (0, 143, 149), (179, 0, 194), (209, 99, 106), (5, 121, 0), (227, 255, 205), (147, 186, 208), (153, 69, 1), (3, 95, 161), (163, 255, 0), (119, 0, 170), (0, 182, 199), (0, 165, 120), (183, 130, 88), (95, 32, 0), (130, 114, 135), (110, 129, 133), (166, 74, 118), (219, 142, 185), (79, 210, 114), (178, 90, 62), (65, 70, 15), (127, 167, 115), (59, 105, 106), (142, 108, 45), (196, 172, 0), (95, 54, 80), (128, 76, 255), (201, 57, 1), (246, 0, 122), (191, 162, 208)]}\n"
     ]
    }
   ],
   "source": [
    "retinanet = init_detector('../configs/detection/retinanet_r50.py','../checkpoints/detection/retinanet_r50.pth', device='cpu')\n",
    "cfg_tr = retinanet.cfg.copy()\n",
    "cfg_tr['val_dataloader']['dataset']['data_root'] = '../data/coco/'\n",
    "cfg_tr['test_dataloader']['dataset']['data_root'] = '../data/coco/'\n",
    "print(cfg_tr['test_dataloader']['dataset'])\n",
    "# cfg_tr['train_dataloader']['batch_size'] = 8\n",
    "# cfg_tr['train_dataloader']['num_workers'] = 1\n",
    "\n",
    "# train_dataloader = Runner.build_dataloader(cfg_tr['train_dataloader']) \n",
    "\n",
    "print(cfg_tr['val_dataloader']['dataset']['ann_file'])\n",
    "val_dataloader = Runner.build_dataloader(cfg_tr['val_dataloader'])\n",
    "\n",
    "test_dataloader = Runner.build_dataloader(cfg_tr['test_dataloader'])\n",
    "print(len(val_dataloader.dataset))\n",
    "\n",
    "cfg_tr['val_evaluator']['ann_file'] = cfg_tr['val_evaluator']['ann_file'].replace('data','../data') if '..' not in cfg_tr['val_evaluator']['ann_file'] else cfg_tr['val_evaluator']['ann_file']\n",
    "val_eval = Runner.build_evaluator(_,cfg_tr['val_evaluator'])\n",
    "print(val_eval.dataset_meta)\n",
    "setattr(val_eval,'dataset_meta',val_dataloader.dataset.metainfo)\n",
    "print(val_eval.dataset_meta)\n",
    "# test_evaluator_cfg = deepcopy(cfg_tr['test_evaluator'])\n",
    "# print(train_dataset_cfg['data_root'],cfg_tr['test_dataloader']['dataset']['ann_file'] )\n",
    "# test_evaluator_cfg['ann_file'] = os.path.join(cfg_tr['test_dataloader']['dataset']['data_root'], test_evaluator_cfg['ann_file'])\n",
    "# print(test_evaluator_cfg)\n",
    "cfg_tr['test_evaluator']['ann_file'] = cfg_tr['test_evaluator']['ann_file'].replace('data','../data') if '..' not in cfg_tr['test_evaluator']['ann_file'] else cfg_tr['test_evaluator']['ann_file']\n",
    "test_eval = Runner.build_evaluator(_,cfg_tr['test_evaluator'])\n",
    "print(test_eval.dataset_meta)\n",
    "setattr(test_eval,'dataset_meta',test_dataloader.dataset.metainfo)\n",
    "print(test_eval.dataset_meta)\n",
    "# print(val_eval.dataset_meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/5000\n",
      "200/5000\n",
      "300/5000\n",
      "400/5000\n",
      "500/5000\n",
      "600/5000\n",
      "700/5000\n",
      "800/5000\n",
      "900/5000\n",
      "1000/5000\n",
      "1100/5000\n",
      "1200/5000\n",
      "1300/5000\n",
      "1400/5000\n",
      "1500/5000\n",
      "1600/5000\n",
      "1700/5000\n",
      "1800/5000\n",
      "1900/5000\n",
      "2000/5000\n",
      "2100/5000\n",
      "2200/5000\n",
      "2300/5000\n",
      "2400/5000\n",
      "2500/5000\n",
      "2600/5000\n",
      "2700/5000\n",
      "2800/5000\n",
      "2900/5000\n",
      "3000/5000\n",
      "3100/5000\n",
      "3200/5000\n",
      "3300/5000\n",
      "3400/5000\n",
      "3500/5000\n",
      "3600/5000\n",
      "3700/5000\n",
      "3800/5000\n",
      "3900/5000\n",
      "4000/5000\n",
      "4100/5000\n",
      "4200/5000\n",
      "4300/5000\n",
      "4400/5000\n",
      "4500/5000\n",
      "4600/5000\n",
      "4700/5000\n",
      "4800/5000\n",
      "4900/5000\n",
      "5000/5000\n",
      "07/26 16:24:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.80s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=22.62s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=6.36s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.390\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.594\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.417\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.234\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.427\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.522\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.554\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.554\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.554\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.380\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.588\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.693\n",
      "07/26 16:25:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.390 0.594 0.417 0.234 0.427 0.522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coco/bbox_mAP': 0.39,\n",
       " 'coco/bbox_mAP_50': 0.594,\n",
       " 'coco/bbox_mAP_75': 0.417,\n",
       " 'coco/bbox_mAP_s': 0.234,\n",
       " 'coco/bbox_mAP_m': 0.427,\n",
       " 'coco/bbox_mAP_l': 0.522}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "retinanet = retinanet.to(device)\n",
    "retinanet.eval()\n",
    "for i,data in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        data = to_device(data, device)\n",
    "        data = retinanet.data_preprocessor(data)\n",
    "        # print(data['inputs'].shape)\n",
    "        # x = torch.tensor(data['inputs'][0], device = device)\n",
    "        # samples = data['data_samples']\n",
    "\n",
    "        out = retinanet.predict(data['inputs'], data['data_samples'])\n",
    "        test_eval.process(out)\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'{i+1}/{len(test_dataloader)}')\n",
    "\n",
    "test_eval.evaluate(len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetDataPreprocessor()\n",
      "100/5000\n",
      "200/5000\n",
      "300/5000\n",
      "400/5000\n",
      "500/5000\n",
      "600/5000\n",
      "700/5000\n",
      "800/5000\n",
      "900/5000\n",
      "1000/5000\n",
      "1100/5000\n",
      "1200/5000\n",
      "1300/5000\n",
      "1400/5000\n",
      "1500/5000\n",
      "1600/5000\n",
      "1700/5000\n",
      "1800/5000\n",
      "1900/5000\n",
      "2000/5000\n",
      "2100/5000\n",
      "2200/5000\n",
      "2300/5000\n",
      "2400/5000\n",
      "2500/5000\n",
      "2600/5000\n",
      "2700/5000\n",
      "2800/5000\n",
      "2900/5000\n",
      "3000/5000\n",
      "3100/5000\n",
      "3200/5000\n",
      "3300/5000\n",
      "3400/5000\n",
      "3500/5000\n",
      "3600/5000\n",
      "3700/5000\n",
      "3800/5000\n",
      "3900/5000\n",
      "4000/5000\n",
      "4100/5000\n",
      "4200/5000\n",
      "4300/5000\n",
      "4400/5000\n",
      "4500/5000\n",
      "4600/5000\n",
      "4700/5000\n",
      "4800/5000\n",
      "4900/5000\n",
      "5000/5000\n",
      "07/26 16:28:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.55s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=21.86s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=6.79s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.390\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.594\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.417\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.234\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.427\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.522\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.554\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.554\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.554\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.380\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.588\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.693\n",
      "07/26 16:28:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.390 0.594 0.417 0.234 0.427 0.522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coco/bbox_mAP': 0.39,\n",
       " 'coco/bbox_mAP_50': 0.594,\n",
       " 'coco/bbox_mAP_75': 0.417,\n",
       " 'coco/bbox_mAP_s': 0.234,\n",
       " 'coco/bbox_mAP_m': 0.427,\n",
       " 'coco/bbox_mAP_l': 0.522}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_retina = split_retina.to(device)\n",
    "split_retina.eval()\n",
    "# split_model_2 = split_model_2.to(device)\n",
    "# split_model_2.eval()\n",
    "# split_model.cut_point = 1\n",
    "print(split_retina.frontend_preprocessor)\n",
    "for i,data in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "    # break\n",
    "    # data = to_device(data, device)\n",
    "    # print(data)\n",
    "    # data = mask2former_swin.data_preprocessor(data)\n",
    "    # print(data)\n",
    "        # feats = mask2former_swin_split.frontend_preprocessor(data)\n",
    "        # print(feats)\n",
    "        # out =  mask2former_swin_split.backbone.split_forward_v2(feats['inputs'], output_layer = mask2former_swin_split.cut_point-1)\n",
    "        #     # data['inputs'] = {\"hw_shape\": out[0],\n",
    "        #     #                   \"outs\": out[1]}\n",
    "        # feats['inputs'] = SplitSwinFeatures(outs=out[1], hw_shape=out[0], device=device)\n",
    "        feats = split_retina.feature_frontend(data)\n",
    "        # assert len(feats['inputs'])== len(data['inputs']), f'length of feats is {len(feats[\"inputs\"])} != length of data[\"inputs\"] which is {len(data[\"inputs\"])}'\n",
    "        # print([t.shape for t in feats['inputs'].outs])\n",
    "        \n",
    "        # break\n",
    "        # loss = split_model.backend_loss(feats)\n",
    "        # print(len(feats['inputs'].outs))\n",
    "        out = split_retina.backend_inference(feats)\n",
    "        # break\n",
    "        test_eval.process(out)\n",
    "        # break\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'{i+1}/{len(test_dataloader)}')\n",
    "\n",
    "test_eval.evaluate(len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ../checkpoints/detection/retinanet_r50.pth\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from cfm_task_models.split_utils import NetworkSplitter\n",
    "from mmdet.models import RetinaNet\n",
    "\n",
    "@NetworkSplitter() # checking the use of Networksplitter as a decorator + adding extra init stuff\n",
    "class SplitRetinaNet(RetinaNet):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.check_value = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "split_retina_2 = SplitRetinaNet.create_from_cfg_and_checkpoint(\"../configs/detection/retinanet_r50.py\", '../checkpoints/detection/retinanet_r50.pth', device = device, cut_point = 2)\n",
    "print(split_retina_2.check_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'feature_frontend' in dir(split_retina_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetDataPreprocessor()\n",
      "100/5000\n",
      "200/5000\n",
      "300/5000\n",
      "400/5000\n",
      "500/5000\n",
      "600/5000\n",
      "700/5000\n",
      "800/5000\n",
      "900/5000\n",
      "1000/5000\n",
      "1100/5000\n",
      "1200/5000\n",
      "1300/5000\n",
      "1400/5000\n",
      "1500/5000\n",
      "1600/5000\n",
      "1700/5000\n",
      "1800/5000\n",
      "1900/5000\n",
      "2000/5000\n",
      "2100/5000\n",
      "2200/5000\n",
      "2300/5000\n",
      "2400/5000\n",
      "2500/5000\n",
      "2600/5000\n",
      "2700/5000\n",
      "2800/5000\n",
      "2900/5000\n",
      "3000/5000\n",
      "3100/5000\n",
      "3200/5000\n",
      "3300/5000\n",
      "3400/5000\n",
      "3500/5000\n",
      "3600/5000\n",
      "3700/5000\n",
      "3800/5000\n",
      "3900/5000\n",
      "4000/5000\n",
      "4100/5000\n",
      "4200/5000\n",
      "4300/5000\n",
      "4400/5000\n",
      "4500/5000\n",
      "4600/5000\n",
      "4700/5000\n",
      "4800/5000\n",
      "4900/5000\n",
      "5000/5000\n",
      "08/02 14:22:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.64s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=21.04s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=6.16s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.390\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.594\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.417\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.234\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.427\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.522\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.554\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.554\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.554\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.380\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.588\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.693\n",
      "08/02 14:22:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.390 0.594 0.417 0.234 0.427 0.522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coco/bbox_mAP': 0.39,\n",
       " 'coco/bbox_mAP_50': 0.594,\n",
       " 'coco/bbox_mAP_75': 0.417,\n",
       " 'coco/bbox_mAP_s': 0.234,\n",
       " 'coco/bbox_mAP_m': 0.427,\n",
       " 'coco/bbox_mAP_l': 0.522}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_retina_2 = split_retina_2.to(device)\n",
    "split_retina_2.eval()\n",
    "# split_model_2 = split_model_2.to(device)\n",
    "# split_model_2.eval()\n",
    "# split_model.cut_point = 1\n",
    "print(split_retina_2.frontend_preprocessor)\n",
    "for i,data in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "    # break\n",
    "    # data = to_device(data, device)\n",
    "    # print(data)\n",
    "    # data = mask2former_swin.data_preprocessor(data)\n",
    "    # print(data)\n",
    "        # feats = mask2former_swin_split.frontend_preprocessor(data)\n",
    "        # print(feats)\n",
    "        # out =  mask2former_swin_split.backbone.split_forward_v2(feats['inputs'], output_layer = mask2former_swin_split.cut_point-1)\n",
    "        #     # data['inputs'] = {\"hw_shape\": out[0],\n",
    "        #     #                   \"outs\": out[1]}\n",
    "        # feats['inputs'] = SplitSwinFeatures(outs=out[1], hw_shape=out[0], device=device)\n",
    "        feats = split_retina_2.feature_frontend(data)\n",
    "        # assert len(feats['inputs'])== len(data['inputs']), f'length of feats is {len(feats[\"inputs\"])} != length of data[\"inputs\"] which is {len(data[\"inputs\"])}'\n",
    "        # print([t.shape for t in feats['inputs'].outs])\n",
    "        \n",
    "        # break\n",
    "        # loss = split_model.backend_loss(feats)\n",
    "        # print(len(feats['inputs'].outs))\n",
    "        out = split_retina_2.backend_inference(feats)\n",
    "        # break\n",
    "        test_eval.process(out)\n",
    "        # break\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'{i+1}/{len(test_dataloader)}')\n",
    "\n",
    "test_eval.evaluate(len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/02 14:51:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Loads checkpoint by local backend from path: ../checkpoints/detection/mask_repointsv2_swin_t.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/aharell/.pyenv/versions/3.10.6/lib/python3.10/subprocess.py:1070: ResourceWarning: subprocess 3072325 is still running\n",
      "  _warn(\"subprocess %s is still running\" % self.pid,\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!, save to ../checkpoints/detection/mask_repointsv2_swin_t_converted-d70cb43e.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/models/task_modules/builder.py:39: UserWarning: ``build_assigner`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_assigner`` would be deprecated soon, please use '\n",
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/models/task_modules/builder.py:24: UserWarning: ``build_iou_calculator`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn(\n",
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/models/task_modules/builder.py:46: UserWarning: ``build_sampler`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_sampler`` would be deprecated soon, please use '\n",
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/apis/inference.py:90: UserWarning: dataset_meta or class names are not saved in the checkpoint's meta data, use COCO classes by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ../checkpoints/detection/mask_repointsv2_swin_t_converted-d70cb43e.pth\n",
      "08/02 14:51:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Loads checkpoint by local backend from path: ../checkpoints/detection/mask_repointsv2_swin_t.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/models/task_modules/builder.py:39: UserWarning: ``build_assigner`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_assigner`` would be deprecated soon, please use '\n",
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/models/task_modules/builder.py:24: UserWarning: ``build_iou_calculator`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn(\n",
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/models/task_modules/builder.py:46: UserWarning: ``build_sampler`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_sampler`` would be deprecated soon, please use '\n",
      "/localhome/aharell/.pyenv/versions/3.10.6/lib/python3.10/subprocess.py:1070: ResourceWarning: subprocess 3072327 is still running\n",
      "  _warn(\"subprocess %s is still running\" % self.pid,\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!, save to ../checkpoints/detection/mask_repointsv2_swin_t_converted-d70cb43e.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/models/task_modules/builder.py:39: UserWarning: ``build_assigner`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_assigner`` would be deprecated soon, please use '\n",
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/models/task_modules/builder.py:24: UserWarning: ``build_iou_calculator`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn(\n",
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/models/task_modules/builder.py:46: UserWarning: ``build_sampler`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_sampler`` would be deprecated soon, please use '\n",
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/apis/inference.py:90: UserWarning: dataset_meta or class names are not saved in the checkpoint's meta data, use COCO classes by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ../checkpoints/detection/mask_repointsv2_swin_t_converted-d70cb43e.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/models/task_modules/builder.py:39: UserWarning: ``build_assigner`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_assigner`` would be deprecated soon, please use '\n",
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/models/task_modules/builder.py:24: UserWarning: ``build_iou_calculator`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn(\n",
      "/localhome/aharell/CFM-Task-Models/.venv/lib/python3.10/site-packages/mmdet/models/task_modules/builder.py:46: UserWarning: ``build_sampler`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_sampler`` would be deprecated soon, please use '\n"
     ]
    }
   ],
   "source": [
    "from cfm_task_models.legacy import RepPointsV2MaskDetector, convert_swin_checkpoint_file\n",
    "from cfm_task_models.split_utils import NetworkSplitter\n",
    "from cfm_task_models.split_models import SplitRepPointsV2MaskDetector\n",
    "from mmengine.config import Config\n",
    "\n",
    "@NetworkSplitter()\n",
    "class AutoSplitRepPointsV2MaskDetector(RepPointsV2MaskDetector):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "    @classmethod\n",
    "    def create_from_cfg_and_checkpoint(cls, cfg_path, checkpoint_path, device = 'cpu', cut_point=1):\n",
    "        cfg = Config.fromfile(cfg_path)\n",
    "        # print(cfg.model)\n",
    "        if not 'converted' in checkpoint_path:\n",
    "            dst = checkpoint_path.replace('.pth', '_converted.pth')\n",
    "            checkpoint_path = convert_swin_checkpoint_file(checkpoint_path, dst)\n",
    "        \n",
    "        model_init = init_detector(cfg_path, checkpoint_path, device=device)\n",
    "        model = cls.create_from_instance_and_cfg(model_init, cfg, cut_point = cut_point)\n",
    "        # model.prepare_preprocessing()\n",
    "        return model\n",
    "\n",
    "    def backend_inference(self, data):\n",
    "        result = self.val_step(data)\n",
    "        return result #data['data_samples']\n",
    "    \n",
    "    def backend_loss(self, data):\n",
    "        if isinstance(data['data_samples'][0], DetDataSample):\n",
    "            img_metas = []\n",
    "            gt_bboxes = []\n",
    "            gt_labels = []\n",
    "            gt_bboxes_ignore = []\n",
    "            gt_sem_map= []\n",
    "            gt_sem_weights = []\n",
    "            gt_masks = []\n",
    "            for ds in data['data_samples']:\n",
    "                img_metas.append(ds.metainfo)\n",
    "                gt_bboxes.append(ds.gt_instances.bboxes)\n",
    "                gt_labels.append(ds.gt_instances.labels)\n",
    "                gt_bboxes_ignore.append(ds.ignored_instances.bboxes)\n",
    "                gt_masks.append(ds.gt_instances.masks)\n",
    "                gt_sem_map.append(ds.gt_sem.sem_map)\n",
    "                gt_sem_weights.append(ds.gt_sem.sem_weights)\n",
    "        return self.parse_losses(self.loss(data['inputs'], img_metas, gt_bboxes, gt_labels, \n",
    "                                           gt_bboxes_ignore = gt_bboxes_ignore, \n",
    "                                           gt_masks = gt_masks, \n",
    "                                           gt_sem_map = gt_sem_map,\n",
    "                                           gt_sem_weights = gt_sem_weights))\n",
    "\n",
    "split_rep1 = AutoSplitRepPointsV2MaskDetector.create_from_cfg_and_checkpoint('../configs/detection/mask_reppoitsv2_swin_t.py', '../checkpoints/detection/mask_repointsv2_swin_t.pth', device = device, cut_point = 1)\n",
    "split_rep2 = SplitRepPointsV2MaskDetector.create_from_cfg_and_checkpoint('../configs/detection/mask_reppoitsv2_swin_t.py', '../checkpoints/detection/mask_repointsv2_swin_t.pth', device = device, cut_point = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
