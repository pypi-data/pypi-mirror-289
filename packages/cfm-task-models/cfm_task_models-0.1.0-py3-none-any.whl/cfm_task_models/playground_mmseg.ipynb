{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mim download mmdet --config mask-rcnn_swin-t-p4-w7_fpn_1x_coco --dest .\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmengine.config import Config\n",
    "from cfm_task_models.legacy import *\n",
    "\n",
    "from cfm_task_models.split_utils import SplitSwinTransformer, SplitTwoStageDetector, TwoInputIdentity, SplitRepPointsV2MaskDetector\n",
    "\n",
    "config_file_old = 'obj_det/cfgs/swin_tiny_mask_rcnn_simplified_cfg.py'\n",
    "checkpoint_file_old = 'obj_det/chkpts/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth'\n",
    "config_file = 'obj_det/cfgs/mask_reppointsv2_swin_t_modified.py'\n",
    "checkpoint_file = 'obj_det/chkpts/mask_reppointsv2_swin_tiny_patch4_window7_3x.pth'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model_init = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n",
    "# result = inference_detector(model, 'demo/demo.jpg')\n",
    "\n",
    "# from mmdet.registry import MODELS\n",
    "# MODELS.get('SwinTransformer')\n",
    "# MODELS.get('TwoStageDetector')\n",
    "model = SplitRepPointsV2MaskDetector.create_from_cfg_and_checkpoint(config_file, checkpoint_file).to(device)\n",
    "# model2 = SplitTwoStageDetector.create_from_cfg_and_checkpoint(config_file_old,checkpoint_file_old).to(device)\n",
    "# model.frontend_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.runner import Runner\n",
    "from mmdet.structures import DetDataSample\n",
    "from typing import Union\n",
    "from torch import Tensor\n",
    "def to_device(data_sample:Union[dict,Tensor], device):\n",
    "    if isinstance(data_sample,dict):\n",
    "        for k,v in data_sample.items():\n",
    "            if isinstance(v, (Tensor, DetDataSample)):\n",
    "                data_sample[k] = v.to(device)\n",
    "                # print(f'{k} was sent to {device}')\n",
    "            elif isinstance(v, list):\n",
    "                new_v = [to_device(v2, device) for v2 in v]\n",
    "                data_sample[k] = new_v\n",
    "                # print(f'components of {k} were sent to {device}')\n",
    "            else:\n",
    "                print(f'{k} could not be sent to {device}')\n",
    "    elif isinstance(data_sample, (Tensor, DetDataSample)):\n",
    "        data_sample = data_sample.to(device)\n",
    "        # print(f'input was sent to {device}')\n",
    "    elif isinstance(data_sample, list):\n",
    "        data_sample = [to_device(v, device) for v in data_sample]\n",
    "        # print(f'components of input were sent to {device}')\n",
    "    else:\n",
    "        # print(type(data_sample))\n",
    "        print(f'input could not be sent to {device}')\n",
    "    return data_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cfg_tr = model.cfg.copy()\n",
    "print(cfg_tr['train_dataloader']['dataset']['ann_file'])\n",
    "cfg_tr['train_dataloader']['batch_size'] = 1\n",
    "cfg_tr['train_dataloader']['num_workers'] = 1\n",
    "\n",
    "train_dataloader = Runner.build_dataloader(cfg_tr['train_dataloader']) \n",
    "\n",
    "print(cfg_tr['val_dataloader']['dataset']['ann_file'])\n",
    "val_dataloader = Runner.build_dataloader(cfg_tr['val_dataloader'])\n",
    "\n",
    "test_dataloader = Runner.build_dataloader(cfg_tr['test_dataloader'])\n",
    "print(len(val_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl = runner.train_dataloader\n",
    "# print(model.cut_point)\n",
    "# val_eval = Runner.build_evaluator(_,cfg_tr['val_evaluator'])\n",
    "# setattr(val_eval,'dataset_meta',val_dataloader.dataset.metainfo)\n",
    "from copy import deepcopy\n",
    "test_evaluator_cfg = deepcopy(cfg_tr['test_evaluator'])\n",
    "print(cfg_tr['train_dataloader']['dataset']['data_root'],cfg_tr['test_dataloader']['dataset']['ann_file'] )\n",
    "test_evaluator_cfg['ann_file'] = os.path.join(cfg_tr['test_dataloader']['dataset']['data_root'], test_evaluator_cfg['ann_file'])\n",
    "# # print(test_evaluator_cfg)\n",
    "test_eval = Runner.build_evaluator(_,cfg_tr['test_evaluator'])\n",
    "setattr(test_eval,'dataset_meta',test_dataloader.dataset.metainfo)\n",
    "# print(val_eval.dataset_meta)\n",
    "for i,data in enumerate(train_dataloader):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # data =  to_device(data,device)\n",
    "    # feat = model2.feature_frontend(data)\n",
    "    # print(feat['inputs']['outs'][0].shape)\n",
    "    # out2 = model2.test_step(feat)\n",
    "    # print(data['data_samples'][0])\n",
    "    if isinstance(model, SplitRepPointsV2MaskDetector):\n",
    "        if 'scale_factor' in data['data_samples'][0]:\n",
    "            for d in data['data_samples']:\n",
    "                meta = d.metainfo \n",
    "                # print(d)\n",
    "                meta['scale_factor'] = d.scale_factor * (2 if len(meta['scale_factor']) == 2 else 1)\n",
    "                d.set_metainfo(meta)\n",
    "\n",
    "    # im['inputs'] = torch.stack(im['inputs'])\n",
    "    # print(list(data_.keys()))\n",
    "    # data_ = train_pipeline(im)\n",
    "    # data_ = prp(data_, False)\n",
    "    # data_['inputs'] = model.backbone.split_forward_v2(data_['inputs'], output_layer=model.cut_point-1)\n",
    "    # data_['inputs'] = {\n",
    "    #                \"hw_shape\": data_['inputs'][0],\n",
    "    #\n",
    "    #                \"outs\": data_['inputs'][1]}\n",
    "    # data = prp(data, False)\n",
    "\n",
    "    \n",
    "    feat = model.feature_frontend(data)\n",
    "    # print(feat['inputs']['outs'][0].shape)\n",
    "    loss = model.backend_loss(feat)\n",
    "    if i > 100:\n",
    "        break\n",
    "    # out = model.test_step(feat)\n",
    "    # props = model.backend_raw(feat)\n",
    "    # print(model.cut_point)\n",
    "    # print(model.test_step(feat)[0])\n",
    "    # losses = model.loss(data_['inputs'], data_['data_samples'])\n",
    "    # loss, losses = model.parse_losses(losses)\n",
    "    # test_eval.process(out)\n",
    "# print(out[0].pred_instances.masks)\n",
    "# test_eval.evaluate(i+1)\n",
    "\n",
    "\n",
    "for i,data in enumerate(test_dataloader):\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # data =  to_device(data,device)\n",
    "        # feat = model2.feature_frontend(data)\n",
    "        # print(feat['inputs']['outs'][0].shape)\n",
    "        # out2 = model2.test_step(feat)\n",
    "        if isinstance(model, SplitRepPointsV2MaskDetector):\n",
    "            for d in data['data_samples']:\n",
    "                meta = d.metainfo \n",
    "                meta['scale_factor'] = d.scale_factor * (2 if len(meta['scale_factor']) == 2 else 1)\n",
    "                d.set_metainfo(meta)\n",
    "\n",
    "        # im['inputs'] = torch.stack(im['inputs'])\n",
    "        # print(list(data_.keys()))\n",
    "        # data_ = train_pipeline(im)\n",
    "        # data_ = prp(data_, False)\n",
    "        # data_['inputs'] = model.backbone.split_forward_v2(data_['inputs'], output_layer=model.cut_point-1)\n",
    "        # data_['inputs'] = {\n",
    "        #                \"hw_shape\": data_['inputs'][0],\n",
    "        #\n",
    "        #                \"outs\": data_['inputs'][1]}\n",
    "        # data = prp(data, False)\n",
    "\n",
    "        \n",
    "        feat = model.feature_frontend(data)\n",
    "        # print(feat['inputs']['outs'][0].shape)\n",
    "        out = model.test_step(feat)\n",
    "        # props = model.backend_raw(feat)\n",
    "        # print(model.cut_point)\n",
    "        # print(model.test_step(feat)[0])\n",
    "        # losses = model.loss(data_['inputs'], data_['data_samples'])\n",
    "        # loss, losses = model.parse_losses(losses)\n",
    "        test_eval.process(out)\n",
    "# print(out[0].pred_instances.masks)\n",
    "test_eval.evaluate(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eval = Runner.build_evaluator(_,cfg_tr['val_evaluator'])\n",
    "setattr(val_eval,'dataset_meta',val_dataloader.dataset.metainfo)\n",
    "\n",
    "# test_evaluator_cfg = deepcopy(cfg_tr['test_evaluator'])\n",
    "# print(train_dataset_cfg['data_root'],cfg_tr['test_dataloader']['dataset']['ann_file'] )\n",
    "# test_evaluator_cfg['ann_file'] = os.path.join(cfg_tr['test_dataloader']['dataset']['data_root'], test_evaluator_cfg['ann_file'])\n",
    "# print(test_evaluator_cfg)\n",
    "test_eval = Runner.build_evaluator(_,cfg_tr['test_evaluator'])\n",
    "setattr(test_eval,'dataset_meta',test_dataloader.dataset.metainfo)\n",
    "# print(val_eval.dataset_meta)\n",
    "# print(out)\n",
    "test_eval.process(out)\n",
    "test_eval.evaluate(1)\n",
    "print(test_eval.metrics[0].results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Instances()\n",
    "\n",
    "fk = torch.Tensor([[[3,4]],[[4,5]],[[5,6]]])\n",
    "device = 'cuda'\n",
    "a.fpn_levels = torch.cat([torch.tensor([i]*r[:,0].numel()).to(device) for i,r in enumerate(fk)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "mx = feat['inputs']['outs'][0][0,:].max().item()\n",
    "mn = feat['inputs']['outs'][0][0,:].min().item()\n",
    "f_np = feat['inputs']['outs'][0][0,:].reshape(2048,-1).cpu().detach()\n",
    "f_np = f_np.detach().numpy()\n",
    "# print(f_np)\n",
    "plt.hist((f_np-mn)/(mx-mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat2 = feat['inputs']['outs'][0].view(-1, *feat['inputs']['hw_shape'],\n",
    "#                                 model.backbone.num_features[0]).permute(0, 3, 1,\n",
    "#                                                                 2).contiguous()\n",
    "# print(feat2.shape)\n",
    "norm0 = getattr(model.backbone, 'norm0')\n",
    "feats_to_comp  = norm0( feat['inputs']['outs'][0]).view(-1, *feat['inputs']['hw_shape'],\n",
    "                                model.backbone.num_features[0]).permute(0, 3, 1,\n",
    "                                                                2).contiguous()\n",
    "print(feats_to_comp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in vdl:\n",
    "    # model.cut_point = 0\n",
    "    # model.data_preprocessor = model.frontend_preprocessor\n",
    "    # print(model.val_step(data))\n",
    "    # model.cut_point = 1\n",
    "    # model.data_preprocessor = TwoInputIdentity()\n",
    "    data_ = model.frontend_preprocessor(data, False)\n",
    "    print(data_['data_samples'])\n",
    "    break\n",
    "    data_['inputs'] = model.backbone.split_forward_v2(data_['inputs'], output_layer=model.cut_point-1)\n",
    "    data_['inputs'] = {#\"x\": data_['inputs'][0],\n",
    "                    \"hw_shape\": data_['inputs'][0],\n",
    "                    \"outs\": data_['inputs'][1]}\n",
    "    print(len(data_['inputs']['outs']))\n",
    "    print(data_)\n",
    "    # data_['inputs'] = [data_['inputs']]\n",
    "    # data_['data_samples'] = [data_['data_samples']]\n",
    "    # # forward the model\n",
    "    with torch.no_grad():\n",
    "        results = model.backend_inference(data_)[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdl = runner.val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.stages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log2 as log\n",
    "def H(d):\n",
    "    return -d*log(d) + (d-1)*log(1-d)\n",
    "\n",
    "m = 8\n",
    "D = np.linspace(0.1442,0.14421,10000000)\n",
    "# R = 1-D*m/(m-1)\n",
    "# rate = R*log(m)\n",
    "the_rate = log(m) -H(D) - D * log(m-1) - 2\n",
    "print(D[np.where(the_rate < 0)[0][0]])\n",
    "the_rate[np.where(the_rate < 0)[0][0]] + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mmseg.apis import inference_model, init_model, show_result_pyplot\n",
    "import mmcv\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from mmengine.config import Config\n",
    "from mmengine.dataset import Compose\n",
    "\n",
    "from split_utils import SplitEncoderDecoder, TwoInputIdentity\n",
    "\n",
    "config_file = \"../../segmentation/maskformer_swin-t_upernet_8xb2-160k_ade20k-512x512_modified.py\"\n",
    "checkpoint_file = \"../../segmentation/maskformer_swin-t_upernet_8xb2-160k_ade20k-512x512_20221114_232813-f14e7ce0.pth\"\n",
    "\n",
    "\n",
    "# Build the model from a config file and a checkpoint file\n",
    "model_init = init_model(config_file, checkpoint_file, device='cpu') # or device='cuda:0'\n",
    "\n",
    "# test a single image and show the results\n",
    "img = '../../demo/demo.jpg'  # or img = mmcv.imread(img), which will only load it once\n",
    "result = inference_model(model_init, img)\n",
    "# # visualize the results in a new window\n",
    "# # show_result_pyplot(model, img, result, show=True)\n",
    "# # or save the visualization results to image files\n",
    "# # you can change the opacity of the painted segmentation map in (0, 1].\n",
    "# show_result_pyplot(model, img, result, show=True, out_file='result.jpg', opacity=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from split_utils import SplitEncoderDecoder #, get_mean_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the config file\n",
    "cfg = Config.fromfile('../../segmentation/swin-tiny-patch4-window7-in1k-pre_upernet_8xb2-160k_ade20k-512x512.py')\n",
    "\n",
    "# set the data_root of the config file\n",
    "data_root = '../../data/ade/ADEChallengeData2016'\n",
    "cfg.data_root = data_root\n",
    "cfg.test_dataloader.dataset.data_root = data_root\n",
    "cfg.train_dataloader.dataset.data_root = data_root\n",
    "cfg.val_dataloader.dataset.data_root = data_root\n",
    "\n",
    "\n",
    "train_pipeline = [{'type': 'LoadImageFromFile'},\n",
    "                  {'reduce_zero_label': True, 'type': 'LoadAnnotations'},\n",
    "                  {'keep_ratio': True,\n",
    "                   'ratio_range': (0.5, 2.0),\n",
    "                   'scale': (2048, 1024), # (2048, 512)\n",
    "                   'type': 'RandomResize'},\n",
    "                   {'cat_max_ratio': 0.75, 'crop_size': (512, 512), 'type': 'RandomCrop'},\n",
    "                   {'prob': 0.5, 'type': 'RandomFlip'},\n",
    "                   {'type': 'PhotoMetricDistortion'},\n",
    "                   {'type': 'PackSegInputs'}]\n",
    "\n",
    "cfg.train_dataloader.dataset.pipeline = train_pipeline\n",
    "\n",
    "cfg.dump('../../segmentation/swin-tiny-patch4-window7-in1k-pre_upernet_8xb2-160k_ade20k-512x512_modified.py')\n",
    "\n",
    "config_file = \"../../segmentation/swin-tiny-patch4-window7-in1k-pre_upernet_8xb2-160k_ade20k-512x512_modified.py\"\n",
    "checkpoint_file = '../../segmentation/upernet_swin_tiny_patch4_window7_512x512_160k_ade20k_pretrain_224x224_1K_20210531_112542-e380ad3e.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_file = \"../../segmentation/swin-tiny-patch4-window7-in1k-pre_upernet_8xb2-160k_ade20k-512x512_modified.py\"\n",
    "# checkpoint_file = '../../segmentation/upernet_swin_tiny_patch4_window7_512x512_160k_ade20k_pretrain_224x224_1K_20210531_112542-e380ad3e.pth'\n",
    "\n",
    "# model = SplitEncoderDecoder.create_from_cfg_and_checkpoint(config_file, checkpoint_file)\n",
    "\n",
    "# cfg['work_dir'] = './logs'\n",
    "# runner = Runner.from_cfg(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SplitEncoderDecoder.create_from_cfg_and_checkpoint(config_file, checkpoint_file)\n",
    "# dataloader = Runner.build_dataloader(cfg.val_dataloader)\n",
    "\n",
    "# cut point = 0, 1 test\n",
    "\n",
    "\n",
    "model = SplitEncoderDecoder.create_from_cfg_and_checkpoint(config_file, checkpoint_file)\n",
    "# model.set_cut_point(0) # so that input_layer=self.cut_point=0\n",
    "# model.swap_preprocessor()\n",
    "model.zero_grad()\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "\n",
    "dataloader = Runner.build_dataloader(cfg.val_dataloader)\n",
    "val_eval = Runner.build_evaluator(_, cfg.val_evaluator)\n",
    "setattr(val_eval,'dataset_meta', dataloader.dataset.metainfo)\n",
    "\n",
    "\n",
    "    \n",
    "# eval_results = []\n",
    "for i, data in enumerate(dataloader):\n",
    "    print(f\"Processing image {i}\")\n",
    "    features = model.feature_frontend(data)\n",
    "    result_backend_inference = model.backend_inference(features)\n",
    "    val_eval.process(result_backend_inference)\n",
    "    # eval_result = val_eval.offline_evaluate(result_backend_inference)\n",
    "    # eval_results.append(eval_result)\n",
    "    # result_backend_raw = model.backend_raw(data)\n",
    "    print(f\"Processed image {i}\")\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = val_eval.evaluate(len(dataloader))\n",
    "# result = val_eval.offline_evaluate(result_backend_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# img_path = \"/home/data/ade/ADEChallengeData2016/images/validation/ADE_val_00000001.jpg\"\n",
    "# img = cv2.imread(img_path)\n",
    "# cv2.imshow(\"image\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check SplitEncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from split_utils import SplitEncoderDecoder, set_data_root_in_cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(data):\n",
    "    plt.imshow(data['inputs'][0].permute(1,2,0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../configs/segmentation/mask2former_swin-t_8xb2-160k_ade20k-512x512.py\"\n",
    "checkpoint = \"../checkpoints/mask2former_swin-t_8xb2-160k_ade20k-512x512_20221203_234230-7d64e5dd.pth\"\n",
    "data_root = \"/home/data/ade/ADEChallengeData2016\"\n",
    "\n",
    "cut_point = 0\n",
    "\n",
    "cfg = Config.fromfile(config)\n",
    "set_data_root_in_cfg(cfg, data_root)\n",
    "\n",
    "model = SplitEncoderDecoder.create_from_cfg_and_checkpoint(config, checkpoint, cut_point=cut_point)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.train_dataloader.sampler.shuffle = False\n",
    "# cfg.train_dataloader.sampler.type='DefaultSampler'\n",
    "dataloader = Runner.build_dataloader(cfg.val_dataloader)\n",
    "print(len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataloader:\n",
    "    features = model.feature_frontend(data)\n",
    "    result = model.backend_inference(features)\n",
    "    print(result)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataloader:\n",
    "    # show_img(data)\n",
    "    \n",
    "    features = model.feature_frontend(data)\n",
    "    loss = model.backend_loss(features)\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataloader:\n",
    "    \n",
    "    data = model.frontend_preprocessor(data)\n",
    "    result = model.predict(data['inputs'], data['data_samples'])\n",
    "    print(result)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['inputs']['outs'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "b048090fc6d3014cd2bdcc9ea88274151058c974d3dd655c4d5f3eddfe0b65cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
